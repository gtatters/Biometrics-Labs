---
title: "Lab 7 - Comparing two means and violation of assumptions"
output: 
html_document:
    theme: readable
    highlight: haddock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir =  "~/Dropbox/Biometrics Labs/Lab 7 - Comparing Two Means")
```

___

## Lab Objectives:

- Compare the means of two groups
- Understand paired vs. two-sample designs
- Explore data transformation
- Detect violations of assumptions and use R for nonparametric alternatives to one-sample, paired and two-sample t-tests

___


```{r, echo=FALSE}
setwd("~/Dropbox/Biometrics Labs/Lab 7 - Comparing two means/")
```


# Exercise 1: Paired t-test

When comparing two means, if each data point in one group is uniquely paired with a data point in the other group, then the data are said to be paired. A paired t-test examines the mean difference between the members of each pair. Paired t-tests assume that the differences are normally distributed and that the pairs are randomly sampled.

A paired sampling design is usually more powerful than an unpaired design, as it reduces extraneous variation between sampling units. However, this sampling method is not always possible. When data are not paired, we compare the means of two groups using a two-sample t-test. The two-sample t- test assumes that there is an independent random sample from each group, and that the variable is normally distributed in each population with the same variance.

Welch's t-test is similar to a two-sample t-test, but does not require the assumption of equal variances. The data output from a two-sample t-test in R allows you to specify if to assume equal variances, or not.

Biological studies often examine the same patients before and after a particular treatment. These types of studies can be analyzed using a paired t-test. Let’s begin with an example of weight gain in anorexia patients as a function of family therapy. We want to see if the difference in weight before and after a family therapy intervention is significantly different from zero.

___

## Anorexia therapy

Open the file anorexia family therapy.csv:

```{r}
d<-read.csv("anorexia family therapy.csv")
str(d)
```

Examine the correlation between the BEFORE and AFTER data using the __cor.test()__ function.

Use the __t.test()__ function, and select the paired t-test option by setting paired=TRUE, to examine the difference between the BEFORE and AFTER:

```{r}
cor.test(x=d$BEFORE, y=d$AFTER)
t.test(x=d$BEFORE, y=d$AFTER, paired=TRUE, var.equal=TRUE)
```

The first output displays the correlation statistic. We haven’t covered correlation in class yet; however it is fairly easy to interpret. The correlation coefficient is a measure of the strength and direction of a linear relationship between the two variables. The correlation coefficient can range from -1 to +1, with -1 indicating a perfect negative correlation, and +1 indicating a perfect positive correlation, and 0 indicating no correlation at all. This data shows a correlation of 0.538, indicating that the two variables are positively correlated, but not perfectly. 0.538 is a numerical description of how tightly around an imaginary linear line the points lie. Even though it is not a perfect correlation, you can see that the correlation is significant, with a P-value of 0.026 (rounded to 0.03).

Examine the t.test output. This output contains summary statistics, including mean, df, significance (P-value) for a two-tailed test, and confidence intervals for the difference in weight before and after therapy.  The mean difference is negative because it was calculated by subtracting the weight after therapy from the weight before. The 95% confidence interval of the difference is indicated, and shows that we are confident that there is a weight gain with therapy, as both the lower and upper limits are negative values.

Remember from class that a paired t-test is identical to a one-sample t-test on the differences. Let’s check this using the data in the gain column, using a mu=0, as you are testing the null hypothesis that weight does not change with family therapy:

```{r}
t.test(d$GAIN, mu=0)
```

Verify that the two approaches lead to identical conclusions and statistical parameters.

<br>

#### Answer question 1 on Sakai

```{r, echo=FALSE}
# 1. The mean weight gain with family therapy is 7.265. This change (is/is not) is statistically significant at alpha = 0.05. The results of the paired samples test and the one-sample test (are/are not) are the same.

```

<br>

___

## Exercise 2: Two-sample t-test

Often in Biology, you will want to compare means of two independent samples. In this case each measurement is not directly paired with another measurement. Instead we have several measurements in each group, and we want to compare the mean of one group to the other.

## Spinocerebellar ataxia

The data in the file ataxia.sav are from an experiment on a mouse model of the neurodegenerative disease Spinocerebellar ataxia type 1. Researchers examined whether life span differed between mice with no exercise, compared with mice with a mild exercise regime.

Open the file ataxia.csv, peform a normal __t.test()__, but this time we will make use of the formula (~) to indicate the groups you want to compare, and _data_ to indicate the data.frame where the data are located:

```{r}
d<-read.csv("ataxia.csv")
t.test(lifespan ~ treatment, data=d)
```

In this case the test variable is lifespan and the grouping variable is treatment. This ensures we will be comparing the mean lifespan (test variable) between the two treatment groups.

In this file the treatment groups are assigned numeric values of 0 or 1, and the values are then labelled no exercise or exercise.  R treats categorical data quite specifically, but behind the scenes, it converts categories to a form of numeric value for coding.  Going forward, if you are unsure about what numeric values are assigned to your groups, consult the __str()__ or the __levels()__ functions:

```{r}
str(d)
levels(d$treatment)
plot(lifespan~treatment, data=d)
```

Notice the plot function also takes categorical data as a grouping variable, creating a box and whisker plot from your data.  From this graph, you might suspect that the variances of the two groups are not the same.  Unfortunately, R expects you to know this, so you will need to install the package <car>, using the __install.packages("car")__ command, in order to perform a Levene Test for variance homogeneity:

```{r}
# install.packages("car")
library(car)
leveneTest(d$lifespan, d$treatment)

```

Remember that an important assumption of a two-sample t-test is that the two groups have equal variances. Levene’s test checks this. If the results of Levene’s test are significant (P<0.05), this indicates that the two groups DO NOT have equal variances. If that were the case, we would need to adjust the t.test inputs to reflect this.  Compare the results of the t.test with different variance assumptions: 

```{r}
t.equal<-t.test(lifespan ~ treatment, data=d, var.equal=TRUE)
t.equal
t.unequal<-t.test(lifespan ~ treatment, data=d, var.equal=FALSE)
t.unequal

t.equal$p.value
t.unequal$p.value
```

Welch’s approximate t-test does not assume equal variances, and you should see different degrees of freedom and confidence intervals.

If the results of Levene’s test are not significant (P>0.05), then we can assume that the variances are equal and can use the normal Two Sample t-test. The P-value can be found in the third row of each output, or derived from a result variable as shown above.

<br>

#### Answer question 2 on Sakai

```{r, echo=FALSE}
# 2. The P-value for the lifespan of mice with spinocerebellar ataxia type 1 undergoing a mild exercise regime compared to the same mice without an exercise regime is 0.010.
answer2<-t.test(lifespan ~ treatment, data=d, var.equal=TRUE)$p.value
```

<br>

___

# Exercise 3: Choose the right t-test

## Homophobia

In a paper published in 1996, Adams et al examined whether homophobia is associated with homosexual arousal. Men were assigned to either the homophobic or nonhomophobic group based on their scores on a Homophobia Index test. The men then watched sexually explicit erotic homosexual videotapes, and an arousal score was determined. These data are found in the file homophobia.csv. Conduct a statistical test to determine if there is any difference in arousal between the two groups.

```{r}
d<-read.csv("Homophobia.csv")
str(d)
```

<br>

#### Answer question 3 on Sakai.

```{r, echo=FALSE}
# 3. Homophobic men had a statistically significant higher level of arousal when watching homosexual videos than non-homophobic men. True
answer3<-t.test(Arousal~Group, data=d, var.equal=TRUE)$p.value<0.05
```

<br>

___

## NZ Noggins

The head sizes of 18 New Zealand army recruits were measured with both cardboard and metal calipers. Researchers want to determine whether there is a significant difference between head size when measured with these two methods. The data is found in NZnoggins.csv. Open NZnoggins.csv and conduct an appropriate test to determine if there is a difference in head size when measured with cardboard and metal calipers.

```{r}
d<-read.csv("NZnoggins.csv")
str(d)
```

<br>

#### Answer question 4 on Sakai

```{r, echo=FALSE}
# 4. Which of the following statements about the measurement of the heads of NZ army recruits with cardboard calipers compared to metal calipers is true? C. The mean head measurement is higher with cardboard calipers, and the difference is significant at alpha = 0.05.
result<-t.test(d$CardboardCalipers, d$MetalCalipers, paired=TRUE)
# result
```
<br>

___


## Beer & mosquitos

A study in West Africa on mosquitos that cause malaria examined whether drinking beer influenced attractiveness to mosquitos. Mosquitos were released next to 25 alcohol-free participants, and the proportion of mosquitos flying toward the participants was measured. The procedure was repeated 15 minutes after each of the same participants had consumed a liter of beer. Open mosquitos beer.csv and conduct an appropriate test to determine if there is a change in attractiveness to mosquitos following drinking beer.

```{r}
d<-read.csv("mosquitos beer.csv")
str(d)
```
<br>

#### Answer question 5 on Sakai

```{r, echo=FALSE}
# 5. Which of the following describes the attractiveness of participants to mosquitos before and after drinking beer? D. Participants were more attractive after drinking beer, and this was statistically significant.
diff<-mean(d$afterDrink)-mean(d$beforeDrink)
# diff is positive=more attractive after the beer
beercorrelation<-cor.test(d$beforeDrink, d$afterDrink)
result<-t.test(d$beforeDrink, d$afterDrink, paired=T)
```

<br>

___

## More beer & mosquitos

The West African study was expanded by including a second control group of 18 participants who were given water instead of beer. The change in activation of mosquitos was measured in both the group of 25 participants who drank beer, and the 18 participants who drank water. Open mosquitos beer water.csv and conduct a statistical test to examine the difference in the change in mosquito activation between the beer group and the water group.

```{r}
d<-read.csv("mosquitos beer water.csv")
str(d)
plot(d$change~d$drink)
t.test(d$change~d$drink, data=d)

```

<br>

#### Answer question 6 on Sakai

```{r, echo=FALSE}
# 6. The P-value for the change in the proportion of mosquitos attracted to participants who drank beer compared to participants who drank water is 0.0027.
answer6<-t.test(d$change~d$drink, equal.var=T)$p.value
```

<br>

___


# Exercise 4: Violations of assumptions and nonparametric tests

Many parametric tests, including t-tests, are robust to their assumptions. This means that the test still works well even when the assumptions are violated. For example, with large sample sizes, the two-sample t-test works acceptably even if the two groups’ standard deviations differ by threefold. However, using a parametric test when the assumptions are not met increases the _type I_ error rate (probability of rejecting a true null hypothesis). If the assumptions are violated too severely to ignore, data can sometimes be transformed to make new variables that better match the assumptions of the methods. For example, with the log-transformation, the natural logarithm (Ln) of the value of a variable is computed, and is used in parametric tests instead of the variable itself.

If the assumptions are violated and data transformation doesn’t produce data that better meet the assumptions, then nonparametric tests can be used. These include the Mann-Whitney U test (or Wilcoxon rank sum test), which examines the distributions of two independent groups, and the Sign test, which can be used in place of both a one-sample t-test and a paired samples t-test.  Nonparametric tests are not as powerful as parametric tests. That is, they are less likely to reject a false null hypothesis (higher type II error). It is therefore better to use parametric tests if the assumptions can be met, and turn to nonparametric methods only after data transformation has failed to meet the assumptions of the parametric methods.

A _Shapiro-Wilk_ test is a formal method for testing for departures from normality. A P-value of less than 0.05 indicates that the data are not a good fit for the normal distribution. Formal normality tests should be used with caution, as small samples may not have enough power to reject normality, and large samples may reject normality even when the departure is not severe to give up on methods that assume normality.

<br>

___

## Biology & sports

A group of French researchers was interested in whether people who play a lot of sports have more sexual partners than those who do not. They asked two groups of students how many sexual partners they had in the previous year (Faurie et al 2004). One group was composed of 103 physical education majors who regularly participated in sports, and the other group was composed of 100 biology majors who did not regularly participate in sports. The data is found in the file Biology sports.csv.

```{r}
d<-read.csv("biology sports.csv")
str(d)
```
To examine the data more closely, we'll create two new data.frames using the __subset()__ function to pull out only those records that correspond to each major, creating a biology and a sports data.frame.  This makes plotting histograms of the data easier to do in a basic R tutorial:

```{r}
library(car) # needed for the qqp plots
biology<-subset(d, major=="biology")
sports<-subset(d, major=="sports")
hist(biology$numberSexPartners)
hist(sports$numberSexPartners)
qqp(biology$numberSexPartners)
qqp(sports$numberSexPartners)
```

Examine the two histograms of the data. You should be able to see that the data are not normally distributed. The important consideration for statistical tests is whether the variable is normally distributed in the population. However, random samples that are very skewed are often reflective of a non-normal population distribution.

Next, take a look at the two Normal Q-Q plots. These plots are another method for detecting deviations from normality. If the data are normally distributed, the data points should fall roughly along a straight line. The plots include a straight line as a guide.

We are also interested in the results of the Shapiro-Wilk test. If the Sig. value of this test is less than 0.05, then the data are not a good fit for the normal distribution.

```{r}
shapiro.test(biology$numberSexPartners)
shapiro.test(sports$numberSexPartners)
```

Now that we know the data are not normal, let’s try a data transformation

Since the data include zeros, we will try log transforming the variable + 1. Simply add a +1 in the numeric expression as follows:

```{r}
biology$lnnumber<-log(biology$numberSexPartners+1)
sports$lnnumber<-log(sports$numberSexPartners+1)
```

Another common transformation for data that are skewed right is a reciprocal transformation.

```{r}
biology$recipnumber<-1/(biology$numberSexPartners+1)
sports$recipnumber<-1/(sports$numberSexPartners+1)
```

Finally, let’s try the square root transformation:
```{r}
biology$sqrtnumber<-1/(biology$numberSexPartners+0.5)
sports$sqrtnumber<-1/(sports$numberSexPartners+0.5)
```

Now let’s check to see if any of these transformations improved the normality of our data. 

Examine these results to determine if any of the transformations improved the fit of the data to a normal distribution.

```{r}
shapiro.test(biology$lnnumber)
shapiro.test(sports$lnnumber)
qqp(biology$lnnumber)
qqp(sports$lnnumber)

shapiro.test(biology$recipnumber)
shapiro.test(sports$recipnumber)
qqp(biology$recipnumber)
qqp(sports$recipnumber)

shapiro.test(biology$sqrtnumber)
shapiro.test(sports$sqrtnumber)
qqp(biology$sqrtnumber)
qqp(sports$sqrtnumber)
```

When a deviation from normality is too strong to ignore, and transformation does not meet the assumptions of a two-sample t-test (normal distribution and equal standard deviation), the next option is to try a non-parametric test.

The Wilcoxon rank-sum test (available in R, as __wilcox.test()__) is equivalent to the Mann-Whitney U-test and used to compare samples that do not follow normal distributions.

Note from the __wilcox.test()__ function's help: The literature is not unanimous about the definitions of the Wilcoxon rank sum and Mann-Whitney tests. The two most common definitions correspond to the sum of the ranks of the first sample with the minimum value subtracted or not: R subtracts and S-PLUS does not, giving a value which is larger by m(m+1)/2 for a first sample of size m. (It seems Wilcoxon's original paper used the unadjusted sum of the ranks but subsequent tables subtracted the minimum.)

See here for an explanation of how the tests are basically the same: <https://stats.stackexchange.com/questions/79843/is-the-w-statistic-output-by-wilcox-test-in-r-the-same-as-the-u-statistic>.

The output of the test statistic in R gives you the statistic called Wilcoxon W and the P-value (Asymp. Sig. 2-tailed). 

If the significance value is less than 0.05, then we can conclude that the distribution of the variable (number of sex partners) is not the same between the two majors. If both distributions have the same shape (in this case they do, both are right-skewed) then we can also conclude that the two groups do not have the same median.

```{r}
wilcox.test(biology$lnnumber, sports$lnnumber)
```

<br>

#### Answer question 7 on Sakai

```{r, echo=FALSE}
# 7. Which of the following is true about the reported number of sex partners between sports majors and biology majors? A. Sports majors reported more sex partners. The difference was significant as determined with a Mann-Whitney U test / Wilcoxon rank sum test.
answer7<-wilcox.test(biology$lnnumber, sports$lnnumber)$p.value
```

<br>

___

## Health expenditure

Health expenditure per person from a random sample of 20 countries is found in the file health expenditure.csv. You suspect that this data is not normally distributed. Use R to conduct a natural log transformation of this data, and conduct a Shapiro-Wilk test on both the untransformed and transformed data to determine if this improves the distribution.

```{r}
d<-read.csv("health expenditure.csv")
str(d)
```

<br>

#### Answer question 8 on Sakai

```{r, echo=FALSE}
# 8. A) The Sahpiro-Wilk test indicates that the distribution of health expenditure (blank: is not) normal.
# B) The distribution of the log-transformed data (blank: is) normal, with a P-value of 0.698 on the Shapiro-Wilk test.
# hist(d$expenditure)
# shapiro.test(d$expenditure)
# lnexpenditure<-log(d$expenditure)
# shapiro.test(lnexpenditure)
# qqp(lnexpenditure)
```
<br>

___

## Killer lions

When an intruding male lions take over a pride of females, they often kill most or all of the infants in the group. Following takeover, the stability of the pride is unpredictable, and scientists hypothesized that females may delay ovulation until the uncertainty has passed. To examine this, a study of lions in the Serengeti measured the time to reproduction of female lions after losing cubs to infanticide, compared to the time to reproduction after losing cubs to accidental death. The data are not normally distributed, even following transformation. Conduct an appropriate statistical test on the data in the file Lions.csv to determine if there is a significant difference in reproduction time.

```{r}
d<-read.csv("Lions.csv")
str(d)
```
<br> 

#### Answer question 9 on Sakai

```{r, echo=FALSE}
# 9. A) The exact P-value for time to next cub of female lions who lost a cub to accidental death compared to infanticide is 0.029.
# B) Time to next cub was (blank: longer) for females who lost a cub to infanticide.
d<-read.csv("Lions.csv")
# str(d)
# plot(d$daysToNextCub~d$death) # infanticide > accidental
answer9a<-wilcox.test(daysToNextCub ~ death, data=d)$p.value

```

<br>

___


## Parasitic ducks

Female goldeneye ducks lay eggs in other females’ nests, in addition to the eggs they produce and raise in their own nests. Females lay just one egg per day, so the total time span for all eggs in one nest to hatch can be affected by whether the eggs were laid early or late. To determine whether a female tends to lay her earliest eggs in her own nest, Andersson and Ahlund (2012) measured eggs produced by 14 female goldeneyes and assigned each a parasitism index. The index is negative if she tends to lay her last eggs in another female’s nest, and positive if she tends to lay her last eggs in her own nest. An index of zero indicates that she alternates laying eggs in her nest and the other female’s nest. This data is not normally distributed, and transformation does not make it normal.

To determine whether the parasitism index is significantly different from zero, we could conduct a one-sample sign test. R has a solution, but you'll have to install a new package <BSDA>. 

For the one-sample sign-test, the null hypothesis is that the median (_md_) of the population from which _x_ is drawn is _md_ (we can set to md=0). 

```{r}
d<-read.csv("Parasitic ducks.csv")
str(d)
# the number of scores > 0:
table(d$parasitismFirstIndex>0)

# install.packages("BSDA")
library(BSDA)
SIGN.test(d$parasitismFirstIndex, md=0)
```

The first line performs a simply boolean test of how many of the index scores are > 0 and we return this as a table of FALSE and TRUE values.  

In this case, 12 ducks had an index greater than zero. Is this enough to reject the null hypothesis? Examine the test-statistic output, which provides the P-value. If this is less than 0.05, we reject the null hypothesis and conclude that the parasitism index is significantly different from zero.

<br> 

#### Answer question 10 on Sakai

<br>
```{r, echo=FALSE}
# 10. The duck researchers had hypothesized that a female duck would lay eggs in her own nest first so that her young would be the first to hatch and would get out of the nest quickly. Does the data support their research hypothesis? B. No, while there was a significant difference in the parasitism index, it showed that females lay eggs in their own nests last, since the median of the index is 5 (i.e. positive and different from 0)
answer10a<-SIGN.test(d$parasitismFirstIndex, md=0)$p.value
answer10b<-SIGN.test(d$parasitismFirstIndex, md=0)$estimate
```

___

## Incompatible mates

![Gouldian finches](GouldianFinches.jpeg)

There are two genetically distinct types of Gouldian finches, one having a red face and the other having a black face. Previous experiments have shown that females have a strong preference for mating with males with the same colour face as themselves, and that the offspring are more likely to survive when both parents are the same type. To examine stress caused by having an incompatible mate, researchers measured corticosterone concentration in 43 females following breeding with a compatible male (same colour face), and in the same 43 females following breeding with an incompatible male (different colour face). The data are not normally distributed. We will therefore conduct a paired-samples sign test.

```{r}
d<-read.csv("incompatible mates.csv")
str(d)
```

To run the sign test in R and incorporate the paired nature of the experimental design, you will have to set paired=TRUE:

```{r}
library(BSDA)
SIGN.test(x=d$Compatible, y=d$Incompatible, paired=TRUE)
```

Consult the output to determine if there is a significant difference in the amount of corticosterone produced in females with incompatible vs compatible mates.

<br>

#### Answer question 11 on Sakai

```{r, echo=FALSE}
# 11. Female Gouldian finches produce significantly more corticosterone when they mate with a male who has the same colour face as themselves. False
result<-SIGN.test(x=d$Compatible, y=d$Incompatible, paired=TRUE)
# result$estimate is negative, so the answer above is likely false, depending on the p value
answer11<-(result$estimate>0 & result$p.value<0.05)
```
<br>


___

# Exercise 5: Additional Practice

<br>

___ 

## Contraception prevalence

The data in the file contraceptive prevalence provides the percentage of married women ages 15 – 49 who are practicing, or whose sexual partners are practicing, any form of contraception. Data have been compiled from World Bank for 57 countries for the year 2002 and the year 2012. You are interested in analyzing the data to determine if there is a difference in contraception between these two years.

First, analyze the data to determine whether it violates the assumptions of the t-test that would apply to this data.

Then, analyze the data using the appropriate statistical test to determine whether there is a change in contraception prevalence between 2002 and 2012.

<br>

#### Answer questions 12 & 13 on Sakai

```{r, echo=FALSE}
# 12. Does the contraception prevalence data meet the assumptions of the t-test that would apply to your analysis? No
# 13. According to your analysis of the contraception prevalence data: A. Contraception prevalence is higher in 2012 than 2002, and this difference is statistically significant.
d<-read.csv("Contraception Prevalence.csv")
# str(d)
# shapiro.test(d$YR2002)
# shapiro.test(d$YR2012)
# both are non-normal
# No values are zero so we do not need to worry about error when transforming
lnyr2002<-log(d$YR2002)
lnyr2012<-log(d$YR2012)
# shapiro.test(lnyr2002)
# shapiro.test(lnyr2012)
# log does not fix normality
recipyr2002<-1/d$YR2002
recipyr2012<-1/d$YR2012
# shapiro.test(recipyr2002)
# shapiro.test(recipyr2012)
# recip does not fix normality
sqrtyr2002<-sqrt(d$YR2002)
sqrtyr2012<-sqrt(d$YR2012)
# shapiro.test(sqrtyr2002)
# shapiro.test(sqrtyr2012)
# sqrt does not fix normality
# thus we can only perform a Mann-Whitney / Wilcoxon rank sum test.  
# in this case, the data are paired (before and after), so it will be a signed rank or
# SIGN.test

result<-SIGN.test(x=d$YR2002,y=d$YR2012, paired=TRUE)
# pvalue = 5e-4, and the 2002 values are < 2012 values

```
<br>

___



