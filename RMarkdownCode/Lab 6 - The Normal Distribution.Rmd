---
title: "Lab 6 - The Normal Distribution"
output: 
html_document:
    theme: readable
    highlight: haddock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Dropbox/Biometrics Labs/Lab 6 - Normal Distribution")
```

___

## Lab Objectives:

- Use an online simulation to visualize sampling distributions, properties of the normal distribution, and the central limit theorem.
- Practice using Z-standardization and statistical tables.
- Use R to conduct one-sample t-tests and to calculate 95% confidence intervals.

___

```{r, echo=FALSE}
setwd("~/Dropbox/Biometrics Labs/Lab 6 - Normal Distribution/")
```

# Exercise 1: Visualizing the normal distribution & central limit theorem

The normal distribution is the "bell-shaped curve." It approximates the distribution of many biological variables. It has a single mode equivalent to its mean, and is symmetric around the mean. The normal distribution is fully described by two parameters: the mean and the standard deviation.

The Central Limit Theorem states that the sum or mean of a set of independent and equally distributed values will have a normal distribution, as long as enough values are added together. This means that sample means will be normally distributed if the sample size is large enough, even if the distribution of the variable itself is not normal.

___

## Simulation of sampling distributions

Open the following link: (http://www.zoology.ubc.ca/~whitlock/kingfisher/SamplingNormal.htm)

This page contains an applet that lets you visualize the process of collecting samples and calculating means. The default settings use a population of fish that are on average 106 mm long, with a standard deviation of 30. These values are the true population parameters, which we would not normally know in real life. The variable “length” has a normal distribution. The default sample size is 10 fish, meaning that you are going to collect 10 fish, measure them, and determine mean fish length.

- Click on the button “Sample 1 individual”. This process measures one fish, and begins preparing a histogram using this fish’s measurement in the first graph below the ruler. Click “sample 1 individual again”, and make sure you understand the process. Now click the button “complete sample of 10”.
- 10 fish have now been sampled and measured. Click “calculate mean” to calculate the mean length of fish from this sample of n = 10.
- Now click “complete sample of 10” another time. This process imagines that you catch and measure a new sample of 10 fish. Click “calculate mean” to calculate the mean length from this second sample.
- Repeat the last step at least two more times.
- Notice that the graph lower graph shows the distribution not of individuals, but of sample means. The bar added in the distribution of sample means always corresponds to the mean of the sample collected in the second graph. This is what is known as a sampling distribution.
- Click on the button “calculate many means”.
- Click on the buttons “show population” and “show sampling distribution”. The curve illustrated in the top graph shows the distribution of the fish length variable in the population. The curve in the bottom graph shows the sampling distribution – the distribution of mean length when many different samples of n=10 are collected and mean length is determined for each sample.
- Change the sample size to n = 75. Click “complete sample of 75”, then “calculate many means”. Notice how the graphs differ from the previous example with n = 10.
- Use the slider to change the standard deviation to 40, and then to 20. Notice how the graphs change with these adjustments.
- Finally, make note of how the two curves change when you change the mean to a smaller mean value, and a larger mean value.

<br> 

#### Answer questions 1-2 on Sakai.

<br>

___

## Simulation of the central limit theorem

- Continuing from the simulation website above:
- At the bottom left of your screen, select “topics” then central limit theorem.
- Click on the tutorial button at the bottom left. Work through the steps of the tutorial, which illustrates important concepts about the central limit theorem.

<br>

#### Answer questions 3-5 on Sakai.

<br>

___


# Exercise 2: Z-standardization

Z-standardization converts values from a normal distribution with a known mean ($\mu$) and standard deviation ($\sigma$) into standard normal deviates (Z), using the equation:

### $$Z = \frac{Y-\mu}{\sigma}$$


A standard normal deviate, or Z, tells us how many standard deviations a particular value is from the mean. Z standardization can be used to determine the probability that a randomly selected individual from a normal distribution falls above or below a given value, or within a certain range. It also tells us the proportion of individuals from a normal distribution who fall within a given range. After you have calculated a standard normal deviate, you can use a statistical table for the standard normal (Z) distribution to determine the probability of sampling a value greater than or equal to a given value of Z.

R can be used to compute standard normal deviates (Z scores) for a given variable. Let’s try this now.


```{r, echo=FALSE}
setwd("~/Dropbox/Biometrics Labs/Lab 6 - Normal Distribution")
```

## Baby Birth Weight

Z standardization can only be used for numerical variables that have a normal distribution.  Weight and height tend to be good examples of these. Other continuous numerical variables such as wing length, antenna length, growth rate, and temperature tend to also be a good fit to the normal distribution. The data in the file birthweight.csv contains information on the birth weight of 189 babies, as well as data on various demographic parameters and indicators of the mother’s health.

Verify that the birthweight variable is a relatively good approximation of the normal distribution by plotting a histogram of this variable.

```{r}
d<-read.csv("Birthweight.csv")
hist(d$BirthWeight, breaks=30)
```

Looks reasonable to assume it is normal, at least by the shape.

__Generating Z-Scores__

Normally, to create z-scores (standardized scores) from a variable, you would subtract the mean of all data points from each individual data point, then divide those points by the standard deviation of all points:

```{r}
ZBirthWeight<-(d$BirthWeight - mean(d$BirthWeight))/sd(d$BirthWeight)
```

This can also be accomplished in one call using __scale(x, center, scale)__
where:

- x: a numeric object
- center: if TRUE, the objects’ column means are subtracted from the values in those columns (ignoring NAs); if FALSE, centering is not performed
- scale: if TRUE, the centered column values are divided by the column’s standard deviation (when center is also TRUE; otherwise, the root mean square is used); if FALSE, scaling is not performed

```{r}
BirthWeight.scaled<-scale(d$BirthWeight, center=TRUE, scale=TRUE)
```

You can add a variable back to the entire dataset, d:

```{r}
d$ZBirthWeight<-ZBirthWeight
d$BirthWeight.scaled<-BirthWeight.scaled
summary(d)
```

The __summary()__ function displays descriptive information about each variable (i.e. column) in the dataset.

Consider the mean of the variable ZBirthweight. Examine the ZBirthWeight variable more closely. What do the positive and negative signs on the Zscores indicate?  Unfortunately, the __summary()__ command does not report standard deviation, but you can check that your variable has been standardised if the calculated sd = 1:

```{r}
sd(ZBirthWeight)
```

So, what do the z-scores look like?  They are perfectly correlated with the original data, but you can see that a value of 0 is in the middle of the dataset (i.e. the mean) and a z-score of 1 corresponds to the standard deviation of the data.

```{r}
plot(d$BirthWeight~d$ZBirthWeight, xlab="Z-score", ylab="Birth weight (g)")
lines(c(0,0), c(-2000,2945), col="red")
lines(c(-4,0), c(mean(d$BirthWeight), mean(d$BirthWeight)), col="red")
lines(c(1,1), c(-2000, mean(d$BirthWeight)+sd(d$BirthWeight)), col="blue")
lines(c(-4,1), c(mean(d$BirthWeight)+sd(d$BirthWeight),mean(d$BirthWeight)+sd(d$BirthWeight)), col="blue")
lines(c(-1,-1), c(-2000, mean(d$BirthWeight)-sd(d$BirthWeight)), col="blue")
lines(c(-4,-1), c(mean(d$BirthWeight)-sd(d$BirthWeight),mean(d$BirthWeight)-sd(d$BirthWeight)), col="blue")

```

Consider a situation where you have only the data (sorted in ascending order) found here:

```{r}
head(data.frame(BirthWeight=sort(d$BirthWeight), ZBirthWeight=sort(d$ZBirthWeight)))
```
...
```{r}
tail(data.frame(BirthWeight=sort(d$BirthWeight), ZBirthWeight=sort(d$ZBirthWeight)))
```

and you do not know the mean or standard deviation. How can you estimate the mean? How can you estimate the standard deviation?

Estimating probability of z-scores using R:

```{r}
# Pr(X>z), where X is a number of interest:
1-pnorm(2, mean=0, sd=1) # 2.2275% chance of a number being > 2

# Pr(X<z), where X is a number of interest:
pnorm(-2, mean=0, sd=1)  # 2.2275% chance of a number being < -2


```
<br>

#### Answer questions 6-8 on Sakai.

<br>

```{r, echo=FALSE}
# 6. A negative Z score indicates: The observed value falls below the mean.
# 7. According to the case summaries table, the observed birth weight that is closest to the mean is 2948 g. The observed birth weight that is the closest to 1 standard deviation away from the mean is 2211 g. From these values, we could estimate the standard deviation to be 737 g.

# 8. The highest birth weight in this sample of babies is 4990 g, which is equal to approximately 11 lb. According to Z standardization, what is the probability that a randomly selected newborn baby in this population would have a birth weight greater than 11 lb? 1/189=0.0026

# I get a slightly different answer, but very close:
zpercent<-pnorm(2.80495, lower.tail=F)
zpercent


```

___

## Death Valley temperature

Answer the following question as you would in class, or on a test – that is, using the appropriate formulas and the standard normal distribution table.

The highest recorded temperature during the month of July for a given year in Death Valley, California, has an approximately normal distribution with a mean of 123.8°F and a standard deviation of 3.1°F.

A. What is the probability for a given year that the temperature never exceeds 120°F in July in Death Valley?
B. What is the probability that the temperature goes above 128°F during July in a randomly chosen year?
C. What is the probability that the highest recorded temperature will be between 128°F and 130°F?

<br>

#### Answer questions 9 and 10 on Sakai

```{r, echo=FALSE}
# 9. A) The probability that the temperature never exceeds 120 degrees F in a given July in Death Valley is 0.109.
# 9 B) The probability that the temperature goes above 128 degrees F during July in a randomly chosen year is 0.089.
# 10. The probability that the highest recorded temperature will be between 128 and 130 degrees F is: 0.066.

# 9a
zvalue<-abs(120-123.8)/3.1 # compare z=1.226 to values in table 
# between 1 - 0.8888 and 1 - 0.8907
# or more accurately: 
answer9a<-pnorm(120, mean=123.8, sd=3.1) # 0.1101

#9b
zvalue<-abs(128-123.8)/3.1 # z=1.355
answer9b<-1-pnorm(zvalue) # 0.08773 

# 10
zvalue<-abs(130-123.8)/3.1 # 2: p=0.9772 (lower tail dist from table)
zvalue<-abs(128-123.8)/3.1 # 1.355: p=0.9115 to 0.9131 (lower tail from table)
# .9772-.9115
answer10<-pnorm(130, mean=123.8, sd=3.1)-pnorm(128, mean=123.8, sd=3.1)

# my answers slightly differ.
# this is the challenge of working from tables. 
# TAs may need to go over this with students and verify with instructor of the preferred method, or change answer key to ensure the ranges are allowed

```

<br>

___

# Exercise 3. One-sample t-tests

If a variable Y is normally distributed in the population with a mean $\mu$, and we have a random sample of n individuals, then the sample means are also normally distributed with a mean equal to $\mu$ and a standard error of $\frac{\sigma}{\sqrt{n}}$. 

Z-standardization can be used to calculate the probability of observing a mean of 𝑌 using the formula:

### $$Z = \frac{\overline{Y} - \mu}{\sigma_{\overline{Y}}}$$

However, if we do not know the true standard deviation in the population ($\sigma$), we must estimate it using SE = $\frac{s}{\sqrt{n}}$ 

This leads to a related quantity called Student’s t:

### $$t = \frac{\overline{Y} - \mu}{SE_{\overline{Y}}}$$

The sampling distribution for _t _ is not a normal distribution, but rather a _t _ distribution, which is fatter in the tails than the standard normal distribution. The sample size determines the number of degrees of freedom of the _t _ distribution (_df _= _n_ – 1), and which particular version of the _t _ distribution we need to use. As sample size increases, _t _ becomes more like _Z_.

The confidence interval for a mean assumes that the variable has a normal distribution in the population and that the sample is a random sample. The 95% confidence interval for the population mean is approximately 2 standard errors above and below the sample mean. However, it can be more precisely calculated by multiplying the critical t-value from the appropriate t-distribution by the standard error.

A one-sample t-test compares the observed sample mean with $\mu_{0}$, a specific value for the population mean proposed in the null hypothesis. It calculates the test statistic _t_, and determines the probability of observing a *t*-value as extreme as, or more extreme than, the calculated *t*-value, if the null hypothesis is true, using the appropriate *t*-distribution with *n* – 1 degrees of freedom.

___

## Age on the titanic

Open the file titanic.csv. This file contains information on the passengers of the Titanic. We'll use a one-sample t-test to ask whether the mean age of passengers was significantly different from 18 years old.

Use the __t.test()__ function, supplying your variable of interest and the null $\mu$ reference value (mu=18):

```{r}
d<-read.csv("titanic.csv")
str(d)
t.test(x=d$Age, mu=18)
# or
t.test(x=d$Age - 18, mu=0)

```

The first few lines provides the t statistic, degrees of freedom and p value in the first row. 

The p-value is by default, a two-tailed or two-sided test.

The mean of x is provided, along with with 95% confidence intervals. 

Since the __t.test()__ function is generic, it provides the 95% confidence intervals for x, which is the tested variable.  Note how if we subtract 18 from the Ages, and test against mu=0, we obtain a confidence interval for the difference. 

If there is a significant difference between the data mean and the test value (which in this case we input as 18), the significance value will be less than 0.05.

Determine the 95% confidence interval of the mean age on the titanic.

As a refresher of what you learned in the last lab, conduct a statistical test to determine whether passenger class and survival on the Titanic are associated. Conduct a second test to determine whether gender and survival are associated.

<br>

#### Answer questions 11 – 14 on Sakai

```{r, echo=FALSE}
# 11. The mean age on the titanic is significantly different from 18-years-old. True
# 12. The mean age on the titanic was 31.19 with a 95% confidence interval lower bound of 30.04 and an upper bound of 32.35.
# 13. Passenger class and survival on the Titanic are independent. False
# 14. There is a statistically significant association between gender and survival on the Titanic. True

answer11<-t.test(d$Age, mu=18)$p.value<0.05
answer12a<-t.test(d$Age)$estimate
answer12b<-t.test(d$Age)$conf.int
# plot(d$PassengerClass~d$Survive)
answer13<-chisq.test(table(d$PassengerClass, d$Survive))$p.value>0.05
# plot(d$Sex~d$Survive)
answer14<-chisq.test(table(d$Sex, d$Survive))$p.value<0.05
```

<br>

___

## Malaria & mosquitos

Malaria is spread by mosquitoes. To properly understand how the disease spreads it is essential to understand the biting behavior of mosquitoes. A study in Kenya measured the relative attractiveness of people infected with malaria, compared to the same people after they had been treated with an effective anti-malaria drug. Mosquitoes were given a choice between the target infected person and two others, and the proportion of mosquitoes going to the infected person was recorded. The same experiment was done with the same three people after the infected person was cured. The data in the "Difference" column are the changes in the proportion of mosquitoes biting the infected person as a result of the treatment. Positive numbers indicate that the target person was bit more when they had malaria.

Open the file malaria mosquito.csv

```{r}
d<-read.csv("Malaria mosquito.csv")
str(d)
```

Consider what mean value you would expect to observe for “difference” if mosquitos bit a person equally before and after malaria treatment. 

Conduct a one-sample t-test to determine if the difference observed was significantly different from this value.


## I NEED CLARIFICATION ON THIS QUESTION.  IT IS NOT APPARENT TO ME WHAT MU VALUE TO USE. IT STRIKES ME THAT MU SHOULD BE 0, BUT THEN THE ANSWER I GET IS DIFFERENT FROM THE SPSS OUTPUT, WHICH SEEMS TO COMPARE THE DIFFERENCE VS. THE BEFORE VALUE, AND THIS DOESN'T MAKE SENSE FOR ME.  UNFORTUNATELY THE EXPERIMENTAL DESIGN EXPLANATION IS CONFUSING.


<br> 

#### Answer question 15 on Sakai

```{r, echo=FALSE}
# 15. The difference in mosquito biting before and after treatment is statistically significant and suggests that more mosquitos bite a person when they are uninfected than when they have malaria. False
result<-t.test(d$Difference, mu=0, alternative="two.sided")$p.value
# mean(c(d$MosquitosBefore, d$MosquitosAfter))
result<-t.test(d$Difference, mu=0.3659)
result<-t.test(d$Difference, mu=mean(d$MosquitosBefore))

```

___

## Hurricanes & soil lead

Hurricanes Katrina and Rita caused flooding of large parts of New Orleans, leaving behind large amounts of sediment. Forty-six sites were monitored for soil lead content before and after the hurricanes. While the ratio of soil lead content is not normally distributed, the log ratio has an approximately normal distribution and can be used in statistical analysis. A ratio of 1 is equivalent to a log ratio of 0. Therefore, a negative log ratio indicates a reduction in soil lead content, while a positive log ratio indicates an increase in soil lead content.

Open the file hurricanes.csv.

```{r}
d<-read.csv("hurricanes.csv")
str(d)
```

Determine the mean and 95% confidence interval of the log ratio.
Consider what mean value you would expect to observe for “log ratio” if the mean soil lead content was unchanged by the hurricanes.
Conduct a one-sample t-test to determine if the difference observed was significantly different from this value.

<br>

#### Answer questions 16 – 17 on Sakai

```{r, echo=FALSE}
# what mean value expect for log ratio if the mean lead content unchanged?  
# log(ratio) = 0
# Question 16. The data are consistent with there being a (blank: decrease) in soil lead content following the hurricanes. 
# Question 17. The P-value for the one-sample t-test conducted on the change in soil lead (using the log ratio) is 0.0024
result<-t.test(d$LogRatio, mu=0)
# since the mean logratio is a negative number and below 0, then the soil lead concentration must have decreased (Ratio < 1 will produce a negative log)
# result is significant, with the logratio difference = -0.2743:
answer16<-factor(result$estimate<0, labels="decrease")
answer17<-result$p.value

```

<br>

___


## Hurricanes & blood lead

In the same study, the concentration of lead in the blood of children living in 46 areas was measured before and after the hurricanes. The ratio of blood lead concentrations has an approximately normal distribution.

- Open the file "hurricanes blood lead.csv"
- Use the transform, create a new variable to calculate the ratio variable, which should be equal to the blood lead concentration after the hurricanes, divided by the concentration before the hurricanes.

```{r}
d<-read.csv("hurricanes blood lead.csv")
str(d)
d$ratio<-d$bloodLeadAfter/d$bloodLeadBefore
```

- Determine the mean and 95% confidence interval of the ratio.
- Consider what mean value you would expect to observe for “ratio” if the concentration of blood lead was unchanged by the hurricanes.
- Conduct a one-sample t-test to determine if the observed ratio was significantly different from this value.

<br>

#### Answer question 18 on Sakai

<br>

```{r, echo=FALSE}
# for ratio to remain constant, mu would need to = 1
# 18. According to the data on the concentration of lead in the blood before and after hurricanes Rita and Katrina, the null hypothesis that there is no change in blood lead concentration is (blank: rejected). Blood lead concentration (blank: decreased) following the hurricanes.
answer18a<-factor(t.test(d$ratio, mu=1)$p.value<0.05, labels="rejected")
answer18b<-factor(t.test(d$ratio)$estimate<1, labels="decreased")
```

___

# Exercise 4: Additional practice

## Back to Birthweight

The birthweight data in the file birthweight.csv were collected at Baystate Medical Center in Springfield Massachusetts during 1986. Conduct an appropriate statistical test to determine whether mean birth weight in this data set is significantly different from the current mean birth weight in Canada, as reported by Statistics Canada <http://www.statcan.gc.ca/pub/84f0210x/2009000/t015-eng.htm>

```{r}
d<-read.csv("Birthweight.csv")
```
Use the appropriate function in R to calculate the mean and 95% confidence interval for birth weight comparing mothers who smoked to mothers who did not smoke.

It might help to learn the __subset()__ function to break your data up into variables on which you can work more easily:

```{r}
Birthweight.smoker<-d$BirthWeight[d$Smoke=="Yes"]
Birthweight.nonsmoker<-d$BirthWeight[d$Smoke=="No"]
mean(Birthweight.smoker)
mean(Birthweight.nonsmoker)
```

Use the appropriate function in R to determine whether there is an association between low birth weight (<2500 g) and the presence of uterine irritability in the mother (the variable LowWeight refers to babies <2500 g).


<br>

#### Answer question 19 to 21 on Sakai

```{r, echo=FALSE}
# The mean birth weight observed in the 1986 Springfield Massachusetts data set is significantly different from the hypothesized mean value according to current Canadian data. True
# 20. A) The 95% confidence interval for mean birth weight for mothers who did not smoke is: 2916.66 <μ< 3194.73.
# B) The 95% confidence interval for mean birth weight for mothers who smoked is: 2619.09 <μ< 2924.74.
#  21. There is a statistically significant association between low birth weight and the presence of uterine irritability in the mother. According to a chi square test, 14 mothers had both uterine irritability and low birth weight infants, whereas only 8.7 would be expected if the two variables were independent. True

mu<-3364 # from Can Stats link above
answer19<-t.test(d$BirthWeight, mu=mu)$p.value<0.05
answer20a<-t.test(Birthweight.nonsmoker)$conf.int
answer20b<-t.test(Birthweight.smoker)$conf.int
result<-chisq.test(table(d$LowWeight, d$Uterine))
# result$observed
# result$expected
answer21<-result$p.value<0.05

```


___

## Cat body weight

The file cats.csv contains data on brain weight and body weight of 137 cats in lbs. Body weight is approximately normally distributed. Use R to compute standard normal deviates for the body weight variable. Use these values to answer the question on Sakai.

<br>

#### Answer question 22 on Sakai


```{r}
# 22. According to the computed Z scores for cat body weight and the Z distribution statistical table, what is the probability that a randomly selected cat is between 8 and 10 lbs? 0.309
d<-read.csv("Cats.csv")
Z<-scale(d$BodyWt, center=T, scale=T)
Z10<-(10-mean(d$BodyWt))/sd(d$BodyWt)
Z8<-(8-mean(d$BodyWt))/sd(d$BodyWt)
answer22<-pnorm(Z10, lower.tail=F)-pnorm(Z8, lower.tail=F)
# lookup z scores on table and look up p values and subtract them 

```

<br>

___

## US Cereal Nutrition

The file UScereal General Mills.csv contains data on the nutritional information of 22 different breakfast cereals manufactured by General Mills. A larger survey of 150 US breakfast cereals estimates the average values for certain nutritional variables to be as follows:

```{r, echo=FALSE}
df<-data.frame(Variable=c("Calories", "Protein", "Fat", "Fibre", "Complex Carbohydrates", "Sugars"),
  AverageValue=c("149 kcal", "3.7 g", "1.4 g", "3.8 g", "19.9 g", "10.1 g"))
library(knitr)
knitr::kable(df, col.names=c("Variable", "Average Value"))
```

Assume that these continuous numerical variables are approximately normally distributed in the full “population” of breakfast cereals. Determine whether the observed mean values in the General Mills data set are significantly different from the expected mean values according to the US breakfast cereal data.

<br>

#### Answer question 23 on Sakai

<br>

```{r, echo=FALSE}
# 23. Which variables in the General Mills data set are significantly different from the expected values according to the larger survey of US breakfast cereals? Check all that apply. Protein and Fibre

d<-read.csv("UScereal General Mills.csv")
# t.test(d$Cals, mu=149)$p.val<0.05
# t.test(d$Protein, mu=3.7)$p.val<0.05 # TRUE
# t.test(d$Fat, mu=1.4)$p.val<0.05
# t.test(d$Fibre, mu=3.8)$p.val<0.05 # TRUE
# t.test(d$Carbs, mu=19.9)$p.val<0.05
# t.test(d$Sugars, mu=10.1)$p.val<0.05
answer23<-c("Protein","Fibre")

```

___

### Normal Distribution Table

![Normal Distribution Table](NormalDistributionTable.png)
