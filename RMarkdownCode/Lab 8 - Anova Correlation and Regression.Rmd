---
title: "Lab 8 - ANOVA, Correlation, and Regression"
output: 
html_document:
    theme: readable
    highlight: haddock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir =  "/Users/GlennTattersall/Dropbox/Biometrics Labs/Lab 8 - ANOVA and Regression")
```

___

## Lab Objectives:

- Compare the means of multiple groups using Analysis of Variance (ANOVA)
- Make post-hoc comparisons with the Tukey-Kramer test
- Analyze the association between two numerical variables using correlation and regression
 
___


```{r, echo=FALSE}
setwd("~/Dropbox/Biometrics Labs/Lab 8 - ANOVA and Regression/")
```


# Exercise 1: ANOVA

A __One Way ANOVA__ is an analysis of variance in which there is only one independent variable. It is used to compare mean differences in two or more groups. An ANOVA with two groups is mathematically equivalent to a two-sample two-tailed t-test. A significant ANOVA result implies that at least one group has a different mean from one other group, but does not directly tell you what the significant difference is. Post-hoc comparisons, such as the __Tukey-Kramer__ test, can be used to determine which group or groups are significantly different from the others.

ANOVA assumes equal variance of each group, that each group has a normal distribution of the variable, and that each population is randomly sampled. The __Kruskal-Wallis__ test is a nonparametric test that compares multiple groups. It is often used in place of ANOVA when the assumptions of ANOVA are violated.

## Maternal Role Adaptation

Various factors can influence how well new mothers adapt to a maternal role. Low birth weight of a baby due to premature labour is believed to increase psychological stress and risk of depression in new mothers, reducing maternal role adaptation. In this study, maternal role adaptation was compared in a group of mothers of low birth-weight (LBW) infants, mothers of LBW infants who had an experimental intervention, and mothers of full-term infants. The hypothesis was that mothers of LBW infants in the experimental intervention would adapt to their maternal role as well as mothers of healthy full-term infants, and that each of these groups would adapt better than mothers of LBW infants in the control group. A lower maternal role adaptation score indicates better adaptation.

Open the file maternal role adaptation.csv

```{r}
d<-read.csv("maternal role adaptation.csv")
str(d)
```

Visualise and describe the data requires some handiwork in R:

```{r}
# Strip chart of adaptation by group.
stripchart(Adapt ~ Group, data = d, method = "jitter", vertical = TRUE)

# calculate mean, sd and n using the tapply function
meanAdapt <- tapply(d$Adapt, d$Group, mean)
sdevAdapt <- tapply(d$Adapt, d$Group, sd)
n         <- tapply(d$Adapt, d$Group, length)
data.frame(mean = meanAdapt, std.dev = sdevAdapt, n = n)
```

ANOVA is robust to deviations from normality, particularly when sample sizes are large. It is also robust to departures from the assumption of equal variance, but only if the samples are all large, about the same size, and there is no more than about a tenfold difference among variances. However, it is still a good idea to check whether your data meet these assumptions.  Homeogeneity of variance is simple enough to do with the __leveneTest()__:

```{r}
plot(d$Adapt~d$Group)
library(car)
leveneTest(Adapt ~ Group, data=d)
```

Visual inspection of the boxplot above suggests similar variance within each group, and the Levene test confirms this.

Variance tests like the Levene Test assume the underlying data do not depart from normality.  It does not make sense to examine normality on the raw data, especially if there are obvious groupings within the data, but the __shapiro.test__ only works on simple datasets.  To examine normality of each group in R requires we create a simple function, __foo()__ that will return only the p value from a __shapiro.test()__ and then we aggregate the output on our data using the formula command.  In R terminology, this function is a "wrapper" for another function, since it wraps up an output. 

Note: __foo()__ is commonly used nomenclature for a simple function in coding in R where the coder does not want to create a permanent function with a descriptive name!


```{r}
foo<-function(x) shapiro.test(x)$p.value
aggregate(Adapt ~ Group, data = d, FUN = foo)
setNames(aggregate(Adapt ~ Group, data = d, FUN = foo), c("Group", "p-val"))
```

This returns a table of p values for each Shapiro test run on each group (we use setNames to make the output easier to follow).

Technically, it is the normality of the residuals that really needs to be assessed in an ANOVA, although that may come later.  Generally, tests of normality on the raw data will be underpowered, especially if the sample sizes within each group are small.  In the case here, all p values are > 0.05 above, so the data appear to be consistent with a normal distribution.

Now, run the ANOVA, using the __aov()__ function, and run a __summary()__ on the result:

```{r}
result<-aov(Adapt ~ Group, data=d)
summary(result)
```

We will learn how to calculate the important values in an ANOVA table in class. The table shows you the calculated variance ratio (F) and the corresponding P-value under “Pr(>F)”. A P-value less than 0.05 indicates that mean adaptation score (in this case) differs among treatments. However, this alone does not tell us which group or groups are significantly different.


```{r}
tukeyres<-TukeyHSD(result, conf.level=0.95)
tukeyres
plot(tukeyres, cex.axis=0.5)
```

To determine how the groups differ, view the output above which contains the results of the Tukey-Kramer post hoc test, comparing all pairs of means. It shows the mean difference for each combination of groups, the lower and upper confidence limits of the difference, and the significance value (P-value). For p values < 0.0001, the value will be listed as 0.0000.  

The results of the Tukey-Kramer test are exact when the sample size in every group is the same. If sample sizes are different, then the test is conservative, which means that the probability of making at least one Type I error is lower than the stated alpha.  The p adj refers to adjusted p values for cases for unbalanced designs. 

So in this case, the means for the Full term and LBW experimental groups are different from the Control.

The box and whisker plot we made earlier allows you to visualize the difference between group means using a graphical method. If you examine this plot you can quickly see that the LBW Control group had a higher adaptability score (indicating poorer adaption to a maternal role) than the LBW experimental and Full term groups.

<br>

#### Answer questions 1 and 2 on Sakai

```{r, message=FALSE, echo=FALSE}
# 1. According to a Shapiro-Wilk test, maternal adaptation score is normally distributed in each of the populations. True
# 2. The maternal role adaptation data support the researcher’s hypothesis that mothers of low birth weight infants will adapt to their maternal role better with experimental intervention than without intervention (control). True

# Solution to Question 1
foo<-function(x) shapiro.test(x)$p.value
pvalues<-setNames(aggregate(Adapt ~ Group, data = d, FUN = foo), c("Group", "p-val"))
answer1<-(pvalues$`p-val`>0.05)

# Solution to Question 2
result<-aov(Adapt ~ Group, data=d)
tukeyres<-TukeyHSD(result, conf.level=0.95)

# Alt solution to Question 2 using planned post-hoc comparison.
# What this provides is the linear contrast for the test of interest.  You can 
# then ask what the conf interval of the linear contrast is and if that includes 0
# then there is no difference.  This requires the multcomp package and thus might not
# be taught:

library(multcomp)
d$Group<-factor(d$Group, labels=c("Full_Term", "LBW_Control", "LBW_Exp"))
# convert the dashes to underscores.  R will not like using dashes as it will consider
# them minus signs
lm.maternal<-lm(Adapt ~ Group, data=d)
maternalPlanned<-glht(lm.maternal, linfct = mcp(Group=c("LBW_Exp - LBW_Control = 0")))
# Uncomment these for the final answer:
# confint(maternalPlanned, level=0.95) # -8.053 to -5.133 does not include 0
# summary(maternalPlanned) # p=4e-14.  Highly significant

```
<br>

___

## Circadian clock

In a 1998 study, Campbell and Murphy reported that the human circadian clock can be reset by exposing the back of the knee to light, which was met with much skepticism. A later experiment re- examined the phenomenon using 22 people who were awakened from sleep to receive a 3-hour light treatment of the eyes, knees, or neither (control). Effect on circadian rhythm was determined by measuring melatonin production two days later.  Melatonin secretion is tightly connected to circadian rhythm, with secretion beginning to rise about two hours before a person’s regular bedtime. A negative measurement indicates a delay in melatonin production, while a positive number indicates an advance. The data is found in the file Knees circadian clock.csv. Use a one-way ANOVA to assess whether light-treatment affected circadian rhythm.

```{r}
d<-read.csv("Knees circadian clock.csv")
str(d)
```
<br>

#### Answer questions 3 and 4 on Sakai

```{r, echo=FALSE}
# 3. Which of the following is true about the effect of light treatment on circadian rhythm? B. The group that had light applied to their eyes is significantly different from both the group that had light applied to their knees and the control group.

# 4. A 3-hour exposure of the eyes to light during sleeping delayed melatonin production two days later. True

# Strip chart of shift by treatment
# stripchart(shift ~ treatment, data = d, method = "jitter", vertical = TRUE)

# calculate mean, sd and n using the tapply function
meanShift <- tapply(d$shift, d$treatment, mean)
sdevShift <- tapply(d$shift, d$treatment, sd)
n         <- tapply(d$shift, d$treatment, length)
summ<-data.frame(mean = meanShift, std.dev = sdevShift, n = n)

#summ
resultvariance<-leveneTest(shift~treatment, data=d)
foo<-function(x) shapiro.test(x)$p.value
resultnormality<-aggregate(shift ~ treatment, data = d, FUN = foo)

# seems fine to perform a normal ANOVA
resultAOV<-aov(shift ~ treatment, data=d)
# summary(resultAOV)
TukeyResult<-TukeyHSD(resultAOV)
# TukeyResult

# Answer 3 derived from examining the TukeyHSD output:
# treatment was significant, and the tukey test reveals that eyes were much lower than control, and knees were no different from control. 

# Answer 4 derived from the summ data frame above:
# this question is almost the same as question 3.  just asking if the eye treatment had a significant effect, which is did.
# Alt answer 4:
library(multcomp)
circadianAnova<-lm(shift ~ treatment, data=d)
circadianPlanned <- glht(circadianAnova, linfct = 
			mcp(treatment = c("Eyes - Control = 0")))
# confint(circadianPlanned)
# summary(circadianPlanned)

```

<br>

___

## Memory recall

In a study of memory recall, a group of subjects were asked to read through a list of 27 words. One group was assigned to count the number of letters in each word. Another group was assigned to think of a word that rhymed with each word. The third group was instructed to think of an adjective that could be used to modify each word. An imagery group was asked to form vivid images of each word. These first four groups were not told that they would later have to recall the word list. Finally, the last group (“Intentional”) was asked to memorize the words as best as they could for later recall. Each group reviewed the list of 27 words three times, and was then asked to write down as many words as they could remember. The data is found in the file Recall.csv. Examine the data to determine if there is any difference between the groups.

```{r}
d<-read.csv("Recall.csv")
str(d)
```

<br>

#### Answer questions 5 – 8 on Sakai.


```{r, message=FALSE, echo=FALSE}
# 5. The memory recall data meet the assumptions of a one-way ANOVA test. True
# 6. Which of the following groups had a mean recall of less than 10 words? A. Counting, B. Rhyming
# 7. Which of the following statements about memory recall is true? C. The rhyming and counting methods are significantly less effective than the adjective, intentional, and imagery methods.
# 8. The P-value for the difference between the rhyming method and the intentional method is 0.001.

# Solution to Question 5
equalvariance.pvalue<-leveneTest(Recall ~ Group, data=d)
# equalvariance.pvalue # p=0.61 
foo<-function(x) shapiro.test(x)$p.value
normality.pvalues<-setNames(aggregate(Recall ~ Group, data = d, FUN = foo), c("Group", "p-val"))
# normality.pvalues$`p-val`>0.05 # all p values > 0.05
# So, the answer to Question 5 is TRUE, the data meet the assumptions of ANOVA (we cannot address the assumption of independence statistically, unless we look at the residuals)
# acf(lm(Recall~ Group, d)$residuals)

# Solution to Question 6
# calculate mean, sd and n using the tapply function
meanRecall <- tapply(d$Recall, d$Group, mean)
summ       <-data.frame(mean = meanRecall)
answer6<-levels(d$Group)[summ$mean<10]
# answer6 # "Counting" "Rhyming" 

# Solution to Question 7
# NOTE: NEED TO CONSULT WITH LESSON PLAN, SINCE I'D HAVE TO ADD MULTCOMPVIEW TO THE LESSONS ABOVE TO MAKE THIS QUESTION APPROACHABLE IN R.  IT's EASY, BUT NOT INCLUDED IN BASE R
d$Group<-factor(d$Group, labels=c("Adj", "Cnt", "Img", "Int", "Rhm"))
aov1<-aov(Recall ~ Group, d)
tukeyres<-TukeyHSD(aov1, ordered=TRUE)
# plot(tukeyres)
# To answer this I had to resort to a new package, multcomp and multcompView which make it easier to recreate the subset/groups that SPSS creates:
library(multcomp)
library(multcompView)
tukeyres<-glht(aov1, lnfct=mcp(Group="tukey"))
# multcompBoxplot(Recall ~ Group, data=d, horizontal=TRUE)

# Solution to Question 8
tukeyres<-TukeyHSD(aov1, ordered=TRUE)
# tukeyres
# third row is "Int-Rhm" which has diff=5.1 and p adj = 0.0014
# which rounded to 3 decimal places is 0.001
# contrast this to a planned comparison, which gives a much lower p value = 0.00016:
# summary(glht(aov1, linfct=mcp(Group="Int - Rhm = 0")))

```

<br>
___

## Cuckoos Eggs

The European cuckoo does not look after its own eggs, but instead lays them in the nests of birds of other species. Do cuckoos lay eggs of different sizes in nests of different hosts? The data file cuckoo eggs.csv contains data on the lengths of cuckoo eggs laid in a variety of other species' nests. Examine the data to determine if there is any difference between the groups. Use a Tukey-Kramer test to determine which pairs of host species are significantly different from each other.

```{r}
d<-read.csv("Cuckoo eggs.csv")
str(d)
```

<br>

#### Answer question 9 on Sakai.

```{r, message=FALSE, echo=FALSE}
# 9. Which of the following statements about cuckoo egg size is true? D. Eggs laid in the nests of Wrens are significantly smaller than the other species.
aov1<-aov(EggLength ~ HostSpecies, data=d)
tukeyres<-TukeyHSD(aov1, ordered=T)
# examine carefully, all of the Wren comparisons are different.  This is not fun to do, mind you, so the boxplot approach here will help, and will show that the wren condition is lower than all others:
# multcompBoxplot(EggLength~HostSpecies, data=d, horizontal=F)
```
<br>

___

# Exercise 2: Kruskal-Wallis test

If the data do not meet the assumptions of the ANOVA test (normal distribution and equal variances in all populations), and the data are not improved by data transformation, a Kruskal-Wallis test can be used instead. The Kruskal-Wallis test is a nonparametric method to compare more than two groups. 

<br>

___

## Malaria & Maize

The pollen of the corn (maize) plant is known to be a source of food for larval mosquitoes of the species Anopheles arabiensis, the main vector of malaria in Ethiopia. The production of maize has increased substantially in certain areas of Ethiopia recently, and over the same time malaria has entered in to new areas where it was previously rare. This raises the question: Is the increase of maize cultivation partly responsible for the increase in malaria?

Data in the file malaria maize.csv contains information on the level of cultivation of maize (low, medium or high) and the rate of malaria per 10,000 people for several sites in Ethiopia. Assume that you know from prior analysis that the incidence rate of malaria is not normally distributed.

Since we know the data are not normally distributed, and our samples are quite small, we will conduct a non-parametric test.

```{r}
d<-read.csv("Malaria maize.csv")
str(d)
ktmaize<-kruskal.test(Incidencerate10000 ~ Maizeyield, data = d)
print(ktmaize)
ktmaize$p.value

```

If the Kruskal–Wallis test is significant, a post-hoc analysis can be performed to determine which groups differ from each other group. 

Probably the most popular host-hoc test for the Kruskal–Wallis test is the Dunn test. The Dunn test can be conducted with the __dunnTest()__ function in the FSA package. 

Because the post-hoc test will produce multiple p-values, adjustments to the p-values can be made to avoid inflating the possibility of making a type-I error.  There are a variety of methods for controlling the familywise error rate or for controlling the false discovery rate.  See ?p.adjust for details on these methods.

The resulting table shows us the significance value for each pairwise comparison, and an adjusted significance value according to the number of pairwise tests made. This adjusted significance helps keep the probability of type I error (α) at 0.05 total across all of the comparisons.

```{r, message=FALSE}
# install.packages("FSA")
library(FSA)
DT <- dunnTest(Incidencerate10000 ~ Maizeyield, data=d, method="bh")
DT
```

When there are many p-values to evaluate, it is useful to condense a table of p-values to a compact letter display format.  In the output, groups are separated by letters.  Groups sharing the same letter are not significantly different.  Compact letter displays are a clear and succinct way to present results of multiple comparisons.

```{r}
### Compact letter display
PT <- DT$res
PT
# install.packages("rcompanion")
library(rcompanion)
cldList(P.adj ~ Comparison, data = PT, threshold = 0.05)
```

<br>

#### Answer question 10 on Sakai

```{r, message=F, echo=F}
# 10. Which of the following statements about the incidence of malaria in different regions of Ethiopia is true? C. There is a significant difference in malaria incidence between regions with low maize production and regions with high maize production only.
DT <- dunnTest(Incidencerate10000 ~ Maizeyield, data=d, method="bh")
# first row contains the information req'd for the answer: 
# DT$res[1,]

```
<br>

___

## Nematode lifespan

An experiment in 2005 examined the effect of the anticonvulsant medication trimethadione on the lifespan of the nematode worm _C. elegans_. The study compared the effect of treatment provided at the larval stage, the adult stage, and both stages, to control worms treated with water only. Examine the data found in the file nematode.csv for deviations from the assumptions of a one-way ANOVA test. If the data violate these assumptions, perform a Kruskal-Wallis test to determine whether any of the treatments had a significantly different lifespan compared with the water control.

```{r}
d<-read.csv("nematode.csv")
str(d)
```
<br>

#### Answer question 11 on Sakai

```{r, echo=FALSE}
# 11. According to the Kruskal-Wallis test, the distribution of nematode lifespan is significantly different between the water treatment (control) and which of the following groups? B. Adult treatment, C. Treatment at both the larval and adult stages.
# plot(lifespan~treatment, d)
# kruskal.test(lifespan ~ treatment, data=d)
DT <- dunnTest(lifespan ~ treatment, data=d, method="bh")
# last 3 rows contain the water contrasts:
#  DT$res[4:6,] 
# You should be able to see that adult treatment - water and treatment at both - water are significantly different

```
<br>

___

# Exercise 3: Correlation

When two numerical variables are associated, we say they are correlated. The correlation coefficient is a quantity that describes the strength and direction of an association. It reflects the amount of “scatter” in a scatter plot of the two variables, but unlike linear regression does not measure how steeply one variable changes when the other changes. The maximum correlation possible is 1.0 or - 1.0. A correlation of 1.0 indicates that the measurements lie along a straight linear line and show a positive relationship (as one variable increases, the other increases as well). In comparison, a correlation of 0.5 will have a generally positive relationship, but a lot more scatter among the points. A correlation of -1.0 indicates that the measurements lie along a straight linear line and show a negative relationship (as one variable increases, the other decreases).

Correlation analysis assumes that the measurements have a bivariate normal distribution, which is a normal distribution in two dimensions rather than one (see Figures below). A bivariate normal distribution occurs when the relationship between X and Y is linear, the cloud of points on a scatter plot has a circular or elliptical shape, and the frequency distributions of X and Y separately are normal. Visualizing a scatter plot of the data is an important step to detect deviations from these assumptions. Histograms depicting the frequency distributions of X and Y separately are also helpful.

### A bivariate normal distribution

```{r MASS ellipse, echo=FALSE}
oldpar<-par()
par(mfrow=c(1,2))
# SIMULATING MULTIVARIATE DATA
# https://stat.ethz.ch/pipermail/r-help/2003-September/038314.html
# lets first simulate a bivariate normal sample
# library(MASS)
# Simulate bivariate normal data
mu <- c(0,0)                         # Mean
Sigma <- matrix(c(1, .5, .5, 1), 2)  # Covariance matrix
# > Sigma
# [,1] [,2]
# [1,]  1.0  0.1
# [2,]  0.1  1.0

# Generate sample from N(mu, Sigma)
bivn <- MASS::mvrnorm(5000, mu = mu, Sigma = Sigma )  # from Mass package
# head(bivn)                                      
# Calculate kernel density estimate
bivn.kde <- MASS::kde2d(bivn[,1], bivn[,2], n = 50)   # from MASS package

# Classic Bivariate Normal Diagram
# library(ellipse)
rho <- cor(bivn)
y_on_x <- lm(bivn[,2] ~ bivn[,1])    # Regressiion Y ~ X
x_on_y <- lm(bivn[,1] ~ bivn[,2])    # Regression X ~ Y
plot_legend <- c("99% CI green", "95% CI red","90% CI blue",
                 "Y on X black", "X on Y brown")

plot(bivn, xlab = "X", ylab = "Y",
     col = "dark blue", cex=0.5,
     main = "Bivariate Normal\nwith Confidence Intervals")
lines(ellipse::ellipse(rho), col="red")       # ellipse() from ellipse package
lines(ellipse::ellipse(rho, level = .99), col="green")
lines(ellipse::ellipse(rho, level = .90), col="blue")
abline(y_on_x)
abline(x_on_y, col="brown")
legend(0,-2.5,legend=plot_legend, cex = .5, bty = "n")


# Three dimensional surface
# Basic perspective plot
col2 <- heat.colors(length(bivn.kde$z))[rank(bivn.kde$z)]
persp(bivn.kde, phi = 30, theta = 30, shade = .1, border = "red",
      xlab="\nX", ylab="\nY", zlab="\nDensity",
      main="Bivariate Normal\nDensity Distribution")  # from base graphics package
par<-oldpar
```

### Examples of data that deviate from a bivariate normal distribution.

```{r, echo=FALSE}
oldpar<-par()
par(mfrow=c(1,2))
bivfunnel <- MASS::mvrnorm(100, mu = mu, Sigma = Sigma )  # from Mass package
bivfunnel[,1]<-abs(bivfunnel[,1])
bivfunnel[,2]<-bivfunnel[,2]*bivfunnel[,1]*1.05
plot(bivfunnel, xlab="X", ylab="Y", main="Funnel", pch=20, col="red")

bivoutlier<-MASS::mvrnorm(100, mu = mu, Sigma = Sigma )
bivoutlier<-rbind(c(5,0), bivoutlier) 
plot(bivoutlier, xlab="X", ylab="Y", main="Outlier", pch=20, col="red")

bivquad<-MASS::mvrnorm(100, mu = mu, Sigma = Sigma )
bivquad[,2]<-bivquad[,2]+bivquad[,1]-bivquad[,1]^2
plot(bivquad, xlab="X", ylab="Y", main="Non-Linear", pch=20,col="red")

bivpattern<-MASS::mvrnorm(100, mu = mu, Sigma = Sigma )
bivpattern[bivpattern[,1]>0,2]<-bivpattern[bivpattern[,1]>0,2]-5
plot(bivpattern, xlab="X", ylab="Y", main="Non-Random Pattern", pch=20,col="red")

par<-oldpar
```

If the assumptions of correlation analysis are violated, then data transformation may be tried. If data transformation does not improve the fit of the data to bivariate normality, then Spearman’s rank correlation is used instead.

<br>

___

## Height & weight by gender

Data comparing the height and weight of a sample of males and females is found in the file Heightweight.csv. Open this file now.
```{r}
d<-read.csv("Heightweight.csv")
str(d)
```

In this case, we know from prior knowledge that the variables height and weight are normally distributed. Construct a scatter plot to determine whether the relationship between the two variables appears to fit a bivariate normal distribution.

```{r}
plot(d$weight~d$height)
```

Now, use the __cor.test()__ function, defining x as height and y as weight. Specify alternative="two.sided" for the significance test.  Note that if you are working with data that violate the assumptions of correlation analysis, you can conduct a Spearman’s rank correlation by setting method="spearman".  By default, method="pearson", and does not need to be indicated except to be explicit. 


```{r}
cor.test(x=d$height, y=d$weight, alternative="two.sided", method="pearson")
```

In the output above, you will first see a summary of the descriptive statistics, followed by the confidence interval for the pearson correlation coefficient. The Pearson Correlation coefficient is a measure of the direction and strength of the relationship between the two variables. The significance value indicates whether or not this association is statistically significant.

The data contains data for both males and females. If we want to conduct subgroup correlations for males and females separately, the easiest way to do this is to subset our data file, into two new dataframes, dm and df, and re-run the analyses above on the separate data frames:

```{r}
dm<-subset(d, sex=="male")
df<-subset(d, sex=="female")

# Will leave these commented out, so you have to run the code yourself:

# cor.test(x=dm$height, y=dm$weight,  alternative="two.sided")
# cor.test(x=df$height, y=df$weight,  alternative="two.sided")

```

The descriptive statistics and correlations should now show up separately for males and females.

Let's go back to the data and create a scatterplot:

```{r}
plot(d$weight~d$height, col=d$sex, xlab="Height", ylab="Weight")
abline(lm(weight~height, d))

corresult2<-cor.test(d$height, d$weight)$estimate^2
corresult2
rsquared<-summary(lm(weight~height, data=d))$r.squared
rsquared
corresult2==rsquared # Are they the same?
```

The __abline()__ function allows you to add a line that is a linear regression through the data.  We will come to linear models later.

The correlation test provides an R statistic.  

A summary of a regression model also provides an $r^2$ statistic. Due to formatting issues, it is often written simply as R2 or R-squared.  Above, we demonstrate that the result from the __cor.test()__ squared is equivalent to the R2 statistic obtained from the __lm()__ command.   The two values are closely related – the correlation coefficient is R, and R-Squared is simply this value squared.

This $r^2$ statistic is used in regression analysis.  It measures the fraction of the variation in Y that is explained by X.

<br>

#### Answer question 12 on Sakai

```{r, message=F, echo=F}
# WILL NEED TO CHANGE OR ADAPT THESE QUESTIONS FOR R.  IT's NOT CLEAR WHETHER THESE QUESTIONS ARE MEANT TO PROBE UNDERSTANDING OR IF THEY ARE SIMPLY MEANT TO SEE THE OUTPUT AND TRANSCRIBE THE ANSWERS ALREADY ON THE SCREEN.  

# 12A) The overall correlation coefficient for height and weight is 0.785.
# 12B) The correlation coefficient for height and weight in males is 0.604.
# 12C) The correlation coefficient for height and weight in females is 0.494. 
# 12D) The R-Square statistic from the scatterplot is 0.616.

roverall<-cor.test(d$height, d$weight, alternative="two.sided")
rmale<- cor.test(x=dm$height, y=dm$weight,  alternative="two.sided")
rfemale<- cor.test(x=df$height, y=df$weight,  alternative="two.sided")
# There is no "scatterplot r squared equivalent output in r since it is a numerical language, not so much graphical", so just fit the model and use summary:
lmoverall<-lm(height~weight, data=d)

answer12a<-roverall$estimate
answer12b<-rmale$estimate
answer12c<-rfemale$estimate
answer12d<-summary(lmoverall)$r.squared

```
<br>

___

## Chocolate & Nobel Prizes

There is evidence that higher consumption of foods containing chemicals called flavonols (including cocoa, red wine, green tea, and some fruits) increases brain function. Messerli (2012) examined whether chocolate consumption in a country is correlated with the number of Nobel prizes. The data are found in chocolate.sav. Begin by constructing a scatter plot to view the data. You should see that the cloud of points are funnel shaped (wider at one end then the other), which indicates deviation from bivariate normality. Imagine that you try many types of data transformation that does not improve the fit of the data to bivariate normality. Conduct an appropriate correlation test to analyze the association between the two variables.



```{r}
d<-read.csv("chocolate.csv")
str(d)
```

<br>

#### Answer question 13 on Sakai.

You may receive a warning message like the following, which is safe to ignore:
```{r, message=FALSE, echo=FALSE, warnings=FALSE}
# 13. The correlation coefficient between chocolate consumption and Nobel prizes is 0.900.
# Solution to Question 13 (uncomment to assist)
# plot(d$nobelPrizesper100million~d$chocolateConsumption)
# hist(d$chocolateConsumption)
# hist(d$nobelPrizesper100million)
# shapiro.test(d$chocolateConsumption)
# shapiro.test(d$nobelPrizesper100million)
# resid<-resid(lm(nobelPrizesper100million~chocolateConsumption, d))
# hist(resid)
# shapiro.test(resid)
# cor.test(d$nobelPrizesper100million, d$chocolateConsumption, method="spearman")
x<-d$nobelPrizesper100million
y<-d$chocolateConsumption
chocnobel<-cor.test(x, y, method="spearman")
answer13<-chocnobel$estimate
```
<br>

___


## ADD, IQ, and GPA

Open the file appendixD.csv, which contains data that you have previously analyzed this term. Calculate the correlation coefficients for ADD score, IQ, GPA and English grade. Note that you can add all variables at the same time, and R will compute the correlation coefficient and significance for each pair of variables. Assume that there are no deviations from bivariate normality in this case.

We will demonstrate this using slightly different sets:

```{r}
d<-read.csv("appendixd.csv")
str(d)
```

How to create a correlation matrix with significance levels (p-value)?

The function __rcorr()__ (in Hmisc package) can be used to compute the significance levels for pearson and spearman correlations. It returns both the correlation coefficients and the p-value of the correlation for all possible pairs of columns in the data table.

Simplified format:

```{r, message=F}
library(Hmisc)
# rcorr(x, type = c("pearson","spearman"))
```

x should be a matrix, where each column corresponds to a variable of interest, and each row corresponds to the value. The correlation type can be either pearson or spearman.

For large datasets, you probably want to only run this on specific columns of interest.  For example, let's select 4 columns of interest put these into a new dataframe, dsub:

```{r, message=F}
dsub<-d[c("gpa","engg", "iq")]

library(Hmisc)
cor1 <- rcorr(as.matrix(dsub))
cor1
```

__rcorr()__ returns two tables, the first is a table of correlation coefficients, where you read row name and column name to ascertain the correlation.  This is often referred to as a correlation matrix.

The second table is a table of p values corresponding to the correlation coefficients above.  Along the diagonal there are no p values since this corresponds to autocorrelation and we do not need to compare the unique variable's correlation with itself.

For the graphically minded, see <https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html>:

```{r, message=FALSE}
library(corrplot)
corrplot(cor(dsub), method="number")
corrplot(cor(dsub), method="ellipse", p.mat=rcorr(as.matrix(dsub))$P, sig.level=0.01)
```
<br>

#### Answer question 14 on Sakai

```{r, message=FALSE, echo=FALSE}
# 14. Which of the following is true about the correlations in the appendix D file? Check all that apply. A. All of the correlations in the table are statistically significant., B. The strongest positive correlation is between GPA in 9th grade and GPA in 9th grade English class., E. IQ score and GPA are positively correlated.

dsub<-d[c("addsc", "iq", "engl", "engg", "gpa")]
library(Hmisc)
cor1 <- rcorr(as.matrix(dsub))
# cor1$P<0.05 # all comparisons p < 0.05
lowestp<-min(cor1$P, na.rm=T) # p = 0.000000000 which is engg vs gpa
# cor1$r # provides all the r estimates.  IQ and GPA is positive = 0.4970

```
<br>

___

# Exercise 4: Regression

Linear regression uses a line to predict a response numerical variable (Y) from a numerical explanatory variable (X). The regression line will take the form Y = a + b X. In this equation, a is the intercept (where the line crosses the axis at X = 0), and b is the slope of this regression line.

A common statistic used in regression analysis is R2. R2 measures the fraction of variance in Y that is predicted by X. If R2 is close to one (the maximum possible value), then X predicts most of the variation in the values of Y, and the Y observations are tightly clustered around the regression line with little scatter.

Hypothesis tests about regression slopes can be made using either a t-test or ANOVA approach. A P- value of less than 0.05 indicates that the slope between the two variables is significantly different from zero.

Linear regression assumes that the true relationship between X and Y is linear, that for every value of X the corresponding values of Y are normally distributed, and that the variance of Y values is the same for all values of X. Non-linearity is best detected by visualizing a scatter plot of the data. Non-linear relationships between X and Y can often be examined by the use of transformations. Non-normality and unequal variance is best examined with a residuals plot, which calculates the difference between every Y data point and the predicted Y according to the regression equation, and plots this against X. A residuals plot should have a roughly symmetric cloud of points above and below zero, a higher density of points close to zero, and no noticeable change from left to right. The plot on the left below fits these requirements well, while the plot on the right does not.


```{r, echo=FALSE}
mu <- c(0,0)                       # Mean
Sigma <- matrix(c(1, 0, 0, 1), 2)  # Covariance matrix

oldpar<-par()
par(mfrow=c(1,2))

bivnorm<-MASS::mvrnorm(1000, mu = mu, Sigma = Sigma )
plot(bivnorm, xlab="X", ylab="Y", main="Random Residual Pattern", pch=20,col="red")

bivfunnel <- MASS::mvrnorm(1000, mu = mu, Sigma = Sigma )  # from Mass package
bivfunnel[,1]<-abs(bivfunnel[,1])
bivfunnel[,2]<-bivfunnel[,2]*bivfunnel[,1]*0.5
plot(bivfunnel, xlab="X", ylab="Y", main="Non Random Residual", pch=20, col="red")

par<-oldpar
```

___

## The Lion Nose

Whitman et al (2004) examined the relationship between age and the proportion of black pigmentation on the noses of 32 male lions of known age in Tanzania. The goal was to be able to predict the age of a lion from the amount of black on its nosepad. Open the file Lions ages.csv to view the data.

```{r}
d<-read.csv("Lions ages.csv")
str(d)
```

It is always good practice to examine your data graphically before conducting statistical analysis. Construct a scatter plot to view the relationship between proportion black (X axis) and lion age (Y axis). Using the __abline()__ function, add a linear fit line. This approach quickly allows you to view the linear equation describing the relationship between two variables, as well as the R2 value.

```{r, echo=FALSE}
plot(d$age~d$proportion.black)
abline(lm(age~proportion.black, d))
```

To perform a regression, we will use the __lm()__ function to fit a linear model.  This function is a work-horse in R for a family of statistical models.  It is usually easiest to create an object to hold your regression analysis.  We will use __lm1__ for starters, setting proportion.black as the independent variable and age as the dependent varible.  The variables are added in this order because we are interested in predicting a lion’s age from the proportion of black pigmentation on its nose, which can be measured.

```{r}
lm1<-lm(age ~ proportion.black, d)
```

The object, lm1, now contains much useful information.  Type summary(lm1) for most of the useful output, coefficients(lm1) for the regression coefficients, confint(lm1) for confidence limits associated with the regression coefficients, fitted(lm1) for the regression fits (or predictions), and residuals(lm1) for the regression residuals:

```{r}
summary(lm1)
coefficients(lm1)
confint(lm1)
anova(lm1)
```

In the summary output, you should see first, the formula "call" to remind you what model you fit.  Below this is a quantile list of your residuals.  Typically you expect your residuals to have a mean or median close to zero and to be roughly symmetrical, thus absolute values of min and max should not be extremely different.  

Then you see a coefficients table, showing the regression coefficients, standard error, t-value statistics (testing whether the estimate is <> 0; i.e. this is a two-tailed t-test for if x = 0).   We should be most interested in this table.

In the first row labelled (Intercept), the value under "Estimate" is the estimated constant value in the equation Y = a + b X (the intercept). The standard error for this value are given. Do not worry about the t-statistic and significance in this row, as we are not interested in the intercept, but rather the slope.

The next row labelled “proportion.black” contains the estimated value for the slope in the "Estimate". The standard error of the slope is also given.

The "proportion.black" row also contains the results of a hypothesis test using the t-test approach.

The t-statistic for the slope is calculated from the formula:

#### $$t = \frac{b - \beta_{0}}{SE_{b}}$$

where b is the estimate of the slope in the sample, $\beta_{0}$ is the null hypothesized value of the slope (usually zero) and $SE_{b}$ is the standard error of the slope in the sample. Since we are only working with two variables, the P-values from both the ANOVA approach and t-test approach should be identical, and the square of the t-statistic should equal the F statistic reported (or very close to).

Beneath the coefficients table are the significance codes, to help categorise the level of significance.

The last 3 lines depict the model residual standard error, the R-squared values, and the F statistics on the overall regression.  The R-squared value should be the same as that obtained from the cor.test(), except squared.  Note that both positive and negative correlations will produce a positive R2 value, which allows them to be compared on the same scale. We will not use adjusted R2values, so you can ignore this information for now.

The ANOVA table contains the results of a hypothesis test using the ANOVA approach. The null hypothesis being tested is that the slope between the two variables is zero (no relationship). The P-value of this test is found under "Pr(>F)" (i.e. Probability the F ratio is greater than a critical F ratio given the degrees of freedom 1,30). If this value is less than 0.05 then there is evidence for a linear relationship between the two numerical variables.

Now let's examine the predictions and residuals:

```{r}
fits<-fitted(lm1)
resids<-residuals(lm1)
```

We have created new values called fits and resids that are derived from the fitted model.  These are also available from the lm1 object:

```{r}
head(lm1$fitted.values)
head(lm1$residuals)
d$age[1]
d$proportion.black[1]
fits[1]
resids[1]
```

For example, the first lion has an age of 1.1, and a proportion black of 0.21. According to the regression formula, a lion with 0.21 black on his nose would be predicted to be 3.11 years old.

The residuals are calculated by subtracting the predicted value of Y from the actual value of Y, for each data point. For example, for the first lion, the predicted age is 3.11 and the actual age is 1.1. Therefore the residual is -2.015.

The residuals data can be used to construct a residuals plot, which lets us detect non-normality and unequal variance. Construct this graph now by plotting proportion black (X axis) against residuals (Y axis). You can add a horizontal line at zero, using the abline(h=0) function. In this case there are fewer points to the right of the graph (high proportion black), which may make the spread of points look different from left to right, but it is not significant enough for us to conclude that the distribution of Y values at each value of X is not normal or has unequal variance.

```{r}
plot(resids ~ d$proportion.black)+abline(h=0)
```

Another way to evaluate the assumption that the residuals are normally distributed is to analyze the residuals data with a normality test and histogram. Try this now, using the qqp or qqnorm function:

```{r, echo=FALSE}
qqp(resids)
qqnorm(resids)
```

One final, powerful shortcut when working with linear models is that you can simply plot the object, which will by default display typical diagnostic residual plots:

```{r}
oldpar<-par()
par(mfrow=c(2,2))
plot(lm1)
par<-oldpar
```

With experience, you will learn to rely on visualisations to diagnose healthy and unhealthy residual plots.  

<br>

#### Answer question 15 and 16 on Sakai

```{r, message=F, echo=F}
# WILL NEED TO CONFIRM THAT THE OUTPUTS I CREATED IN THE EXAMPLES ABOVE ARE TOO DETAILED, SINCE MANY OF THE ANSWERS ARE ALREADY PROVIDED BY MY EXAMPLES. 

# 15A) The slope of the regression line between proportion black on a lion’s nose and the lion’s age is 10.65.
# 15B) According to the linear regression, a lion with a nose containing 0.13 proportion black would be predicted to be 2.26 years old.
# 16. A normality test on the residuals from the lion data suggests that the distribution of the residuals is: A. Normal

# Solution to 15A
lm1<-lm(age~proportion.black, data=d)
coefs<-coefficients(lm1)
answer15A<-coefs[2]

# Solution to 15B
x<-d$proportion.black
# x==0.13 # 4th item
fits<-fitted(lm1)
# fits[4]
predicted<-predict(lm1, newdata=data.frame(proportion.black=0.13))
answer15b<-predicted

# Solution to 16
resids<-lm1$residuals
results<-shapiro.test(resids)
answer16<-factor(results$p.value>0.05, label="Normal")

```
<br>

___


## Brain size

Some nonlinear relationships can be made linear with a suitable transformation. One of the most common transformations is the log transformation. The data in the file mammals.csv contains species name, body mass (in kg) and brain size (in g) of 62 different mammal species. Open this data and construct a scatter plot to visualize brain size (on the Y-axis) compared to body mass. You should recognize immediately that this scatter of points does not look like a normal linear relationship. Transform each variable using the natural log transformation, as you learned last week. Re-plot the data using the log transformed variables. If this improved the linear relationship between the two variables conduct a linear regression analysis on the log-transformed variables.

```{r, message=F}
d<-read.csv("mammals.csv")
str(d)
```

<br>

#### Answer question 17 on Sakai

```{r, message=F, echo=F}
# 17. The R Square value for the linear relationship between log body mass and log brain mass is 0.920. According to the P-value, the slope of the regression line is significantly different from zero.
d$logbodymass<-log(d$bodymasskg)
d$logbrainmass<-log(d$brainmassg)
lm1<-lm(logbrainmass~logbodymass, data=d)
r.squared<-summary(lm1)$r.squared
# examine the p values from the summary table:
# summ<-summary(lm1)

```
<br>

___

## Telomeres

The ends of chromosomes are called telomeres. As individuals age, their telomeres shorten, and there is evidence that shortened telomeres may play a role in aging. Telomeres can be lengthened in germ cells and stem cells by an enzyme called telomerase, but this is not expressed in most healthy somatic cells. (Cancer cells, on the other hand, usually express telomerase.) A set of data collected by Nordfjäll et al. (2005) examined whether there is a relationship between telomere length of fathers and their children. Examine the data in telomeres.csv by regression to determine whether offspring telomere length can be predicted from father telomere length.



```{r}
d<-read.csv("telomeres.csv")
str(d)
```

<br>

#### Answer question 18 and 19 on Sakai

```{r, echo=FALSE}
# 18. In case 11 in the telomere file, the father has a telomere length of 0.506 and the offspring has a telomere length of 0.657. According to the linear regression equation, offspring of a father with a telomere length of 0.506 would be predicted to have a telomere length of 0.688.
# 19. The slope of a linear regression line predicting offspring telomere length from father telomere length is significantly different from zero. True

# Solution to 18
index<-which(d$father.telomere.length==0.506)
observed<-d$offspring.telomere.length[index]

lm1<-lm(offspring.telomere.length ~ father.telomere.length, d)
predicted<-predict(lm1, newdata=data.frame(father.telomere.length=0.506))
answer18<-predicted

# Solution to 19
# obtain by examining the pvalues from:
# summary(lm1)

```
<br>

___

## Stress and mental health

Open the file symptoms and stress.csv. This file contains information on stress levels and severity of mental health symptoms in 106 patients. The authors are interested in whether the severity of mental health symptoms can be predicted from stress levels.

```{r}
d<-read.csv("symptoms and stress.csv")
str(d)
```

<br>

#### Answer question 20 on Sakai.

```{r, echo=FALSE}
# 20. The linear regression equation predicting mental health symptoms from stress levels is: A. Mental health symptoms = 0.688*stress + 74.365

lm1<-lm(symptoms ~ stress, d)
answer20<-coefficients(lm1)

```
<br>

