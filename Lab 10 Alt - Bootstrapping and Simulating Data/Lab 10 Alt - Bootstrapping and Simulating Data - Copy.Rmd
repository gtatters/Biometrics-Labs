---
title: "Lab 10 - Bootstrapping and Simulating Data"
output: 
  html_document:
    theme: readable
    highlight: haddock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir =  "~/Dropbox/Biometrics Labs/Lab 10 Alt - Bootstrapping and Simulating Data")
```

___

## Lab Objectives:

- Sampling from data
- Bootstrapping your data
- Deriving statistical comparisons using bootstrapping
 
___


# Introduction to Bootstrapping

Bootstrapping can be a very useful tool in statistics and it is very easily implemented in R. Bootstrapping comes in handy when there is doubt that the usual distributional assumptions and asymptotic results are valid and accurate. Bootstrapping is a nonparametric method which lets us compute estimated standard errors, confidence intervals and hypothesis testing.

The term 'bootstrapping' originates from the expression "to pick yourself up by your own bootstraps", Imagine yourself stuck in the mud, lifting yourself up and out without external inputs and assistance.  Seems like magic, right?

![Picking yourself up by your own bootstraps](Bootstrap.png)


Generally bootstrapping follows the same basic steps:
  
- Resample a given data set a specified number of times
- Calculate a specific statistic from each sample
- Find the standard deviation (or other measure of interest) of the distribution of that statistic
  
## The sample function

A major component of bootstrapping is being able to resample a given data set, and in R the function which does this is the __sample()__ function.

We'll demonstrate this with a simple analogy of a bag containing 70 marbles of different colours (15 red, 30 blue, 20 green, and 5 white):

```{r}
reds<-rep("red", 15)
blues<-rep("blue", 30)
greens<-rep("green", 20)
whites<-rep("white", 5)
marbles<-c(reds, blues, greens, whites)
marbles
table(marbles)
table(marbles)/5
```

So, the bag of marbles has a ratio of 6:4:3:1 of blue:green:red:white. 

Now sample 10 marbles, at random, but with replacement, and use the __table()__ function to count the colours from the drawn marbles.

Note: Due to differences in the random seed, your results might differ from the results shown below!  We can use the __set.seed()__ function to ensure our example yields similar results.

```{r}
set.seed(1234)
draw<-sample(marbles, size=10)
table(draw)
draw<-sample(marbles, size=10)
table(draw)
draw<-sample(marbles, size=10)
table(draw)
```

Each draw produces a slightly different breakdown of colours, but generally, blue > green > red > white.  If this was done enough times and the results summarised, the ratio would converge to the known ratio.  Of course you could just dump them all out and count them, but that's not bootstrapping!

<br>

___

So, moving away from marbles and back to numbers.  How does the __sample()__ function work with data? 

```{r}
x<-1:100
sample(x, size=10, replace=TRUE, prob=NULL)
```

The first argument is a vector containing the data set to be resampled (in this case, the variable, x). The size option specifies the sample size with the default being the size of the population being resampled. The replace option determines if the sample will be drawn with or without replacement where the default value is FALSE, i.e. without replacement.  The prob option takes a vector of length equal to the data set given in the first argument containing the probability of selection for each element of x. The default value (set prob=NULL) is for a random sample where each element has equal probability of being sampled.  For our applications, we won't likely be adjusting the probability of drawing a particular number, so set prob=NULL.

In a typical bootstrapping situation we would want to obtain bootstrapping samples of the same size as the population being sampled and we would want to sample with replacement.  Since we draw at random, this does not mean each sample is identical to the data.  Indeed, replacement means you might sample that number again at random.  We are merely using the data as the source of information about the distribution, in the absence of any theoretical framework.

Using __sample()__ to generate a permutation of the sequence 1:10:

```{r}
sample(x=1:10, size=10, replace=T)
```

Bootstrap another sample from the same sequence:

```{r}
sample(x=10, size=10, replace=T)
```

___


## Exercise 1: A bootstrap example with fake data

In the following bootstrapping example we would like to obtain a standard error for the estimate of the median.  We will be using the __lapply()__ and __sapply()__ functions in combination with the __sample()__ function.  For more information about the lapply and sapply function please look at the advanced function R library pages or consult the help manuals (?lapply, ?sapply).

### Calculating the standard error of the median

First, we create a fake data set by taking 100 observations from a normal distribution with mean 5 and stdev 3, but we have rounded each observation to nearest integer:

```{r}
data <- round(rnorm(n=100, mean=5, sd=3))
data[1:10] # the first 10 data points
```

Now, let's obtain 1000 bootstrap samples, but just display the first of the bootstrap samples:

```{r}
resamples <- lapply(1:1000, function(i) sample(data, size=100, replace = TRUE))
resamples[1]
```

resamples[1] displays a list of the first bootstrapped sample from the data.

Now we calculate the median for each bootstrap sample, using __sapply()__ which applies the median to each of our 1000 bootstraps, leading to 1000 median values (we'll just display the first 20 here):

```{r}
r.median <- sapply(resamples, median)
r.median[1:20]
```

What does the distribution of medians look like? 

```{r}
hist(r.median, breaks=10)
```

Now we have a distribution of medians, so can calculate the standard deviation of the distribution of medians, using the __sd()__, which is just the square root of the __var()__ function:

```{r}
sd(r.median)
sqrt(var(r.median))
```

We can put all these steps into a single function where all we would need to specify is which data set to use and how many times we want to resample in order to obtain the adjusted standard error of the median.

Create a function which will bootstrap the standard error of the median of your data by any number (num) of bootstraps:

```{r}
b.median <- function(data, num) {
  resamples <- lapply(1:num, function(i) sample(data, replace=T))
  r.median <- sapply(resamples, median)
  std.err <- sqrt(var(r.median))
  list(std.err=std.err, resamples=resamples, medians=r.median)   
}
```

Generate the data to be used (same as in the above example):

```{r}
data1 <- round(rnorm(100, 5, 3))
```

Save the results of the function b.median in the object b1:

```{r}
b1 <- b.median(data1, 30)
```

Display the first of the 30 bootstrap samples

```{r}
b1$resamples[1]
```

Display the standard error:

```{r}
b1$std.err
```

Displaying the histogram of the distribution of medians:

```{r}
hist(b1$medians, breaks=20)
```

We can input the data directly into the function and display the standard error in one line of code:

```{r}
b.median(data=data1, num=1000)$std.err
```

Display the histogram of the distribution of medians:

```{r}
median.boot<-b.median(data=data1, num=50)$medians
hist(median.boot)
```

It would be fairly simple to generalize the function to work for any summary statistic, although there often are packages and programs that can do this for you.

<br>

___


## Using built-in bootstrapping functions

R has numerous built in bootstrapping functions, many of which are found in the __boot__ package.

### Exercise 2: Cats vs. Dogs using the function __boot()__

Bootstrap the ratio of means using the Cats.csv file, containing brain and body weights for a sample of cats.  We might be interested in the ratio of brain to body weight, but prior experience has informed us that ratios and proportions are not necessarily normally distributed data, so we want to use a bootstrap estimation procedure to examine the data. 
Someone has also informed you that the average ratio of body mass to brain mass in dogs is 208, and being a cat lover you want to ascertain a confidence limit to the ratio and potentially compare whether the mean value for dogs lies within that range.
 
Make sure the boot package is installed using install.packages("boot"), if necessary:

```{r}
library(boot)
d<-read.csv("Cats.csv")
str(d)
brainratio<-(d$BodyMass.kg/d$BrainMass.kg)
mean(brainratio)
hist(brainratio, breaks=30)
```

First define the ratio function, which takes the two column variables to calculate the ratio of means, but allows for a weights variable, w:

```{r}
ratio <- function(d, w) sum(d[,2] * w)/sum(d[,1] * w)
```

Then use the __boot()__ function, specifying the data=d, the statistic=ratio (this is what will be bootstrapped using the new ratio function, R=999 bootstrap replicates, and stype="w" to allow weights to be used in the ratio calculation.  By default, the weights will be identical and cancel out, so do not need to be specified.

You can then use the __boot.ci()__ function on the bootstrapped variables to estimate non-parametric confidence limits for the statistic of interest.


```{r}                          
bootratio<-boot(data=d, statistic=ratio, R=999, stype="w")
bootratio
hist(bootratio$t)
boot.ci(bootratio, conf=0.95, type = c("all"))
```


___

## Exercise 3: Bootstrap Hypothesis Test - Speed of Light

Bootstrapping is also used to formally compare data, substituting for parametric statistics that impose assumptions of the data and the population.

The following example is a simple one and commonly used to illustrate bootstrapping.

In 1882 Simon Newcomb performed an experiment to measure the speed of light. The numbers
below represent the measured time it took for light to travel from Fort Myer on the west bank of the Potomac River to a fixed mirror at the foot of the Washington monument 3721 meters away.

28 -44 29 30 26 27 22 23 33 16 24 40 21 31 34 -2 25 19

In the units in which the data are given, the currently accepted “true” speed of light is 33.02. (To convert these units to time in the millionths of a second, multiply by 10e-3 and add 24.8.)


```{r}
speed <- c(28, -44, 29, 30, 26, 27, 22, 23, 33, 16, 24, 29, 24,
           40 , 21, 31, 34, -2, 25, 19)
realspeed<-speed*10e-3+24.8
hist(realspeed)
```

Do the data support the current accepted speed of 33.02?

First, note that the data are not normal, so one cannot perform a t-test:

```{r}
qqnorm(speed)
qqline(speed)
```

To do the t-test we must assume the population of measurements is normally distributed. If this is not true, at best our tests will be approximations.  But with this small sample size, and with such a severe departure from normality, we can’t be guraranteed a good approximation.

The bootstrap offers one approach.

- Step 1: State null and alternative hypotheses:

       $H_0: mean = 33.02$
  
       $H_a: mean <> 33.02$

- Step 2: Set the significance level. We’ll choose 5%.
- Step 3: Choose a test statistic. We wish to estimate the mean speed, and therefore we’ll use the sample average.
- Step 4: Find the observed value of the test statistic: mean(speed)

We now need the p-value, but to do this we need to know the sampling distribution of our test statistic when the null hypothesis is true. We know it is approximately normal, but also that the approximation might not be very good here.

So our approach instead is to perform a simulation under conditions in which we know the null hypothesis is true.

What we’ll do is use our data to represent the population, but first we shift it over so that the mean really is 33.02, but retains the same variance:

```{r}
newspeed <- speed - mean(speed) + 33.02
mean(newspeed)
hist(newspeed)
```

The histogram of newspeed will have exactly the same shape as speed, but it will be shifted over so that it is centered at 33.02 rather than at 21.75.

Now we reach into our fake population and take out 20 observations at random, with replacement. (We take out 20 because that’s the size of our initial sample). We calculate the average. It will be close to 33.02, because that is the mean of the population. We’ll save it, and then repeat this process again. After many repetitions, we’ll have a good idea of what sort of sample averages we should expect when the true mean is 33.02. We will then compare these to our observed sample average (21.75) to see if it’s consistent with our simulated averages. If so, then perhaps the mean really is 33.02. If not, then perhaps Newcomb had some flaws in his experimental design and was measuring incorrectly.

Here’s some code that takes 1000 samples of the data (each of size=20), calculates the mean and stores it:

```{r}
set.seed(1234)
bstrap <- c()
for (i in 1:1000){
   newsample <- sample(newspeed, 20, replace=T)
   bstrap <- c(bstrap, mean(newsample))
   }
hist(bstrap, breaks=100)
```

This distribution doesn’t look normal, which means that we did the right thing (i.e. the bootstrap resembles the shape of the data).  With a larger sample size it would have looked normal (due to the Central Limit Theorem - the distribution of the means of a sample tends to resemble a normal distribution), but 20 apparently isn’t large enough. We can’t, therefore, trust the normal approximation, and our bootstrap approach will be stronger.

As you can see, it’s not impossible for the sample average to be 21.75 even when the true mean is 33.02, since 21.75 is found within the bootstrap distribution. But it’s not all that common, either.

The p-value is the probability of getting something more extreme than what we observed. 
33.02 - 21.75 = 11.27 units away from the null hypothesis. So our p-value is the probability of being more than 11.27 units away from 33.02. Remember Ha was phrased as <> 33.02, so we need to account for both tails: 

This is P(Test Stat < 21.75) + P(Test Stat > 44.29). We don’t know the sampling distribution of our test statistic, but our bootstrap sample lets us estimate this probability by summing up a Boolean test of which values fall outside those ranges, and dividing by the size of the bootstrap:

```{r}
(sum(bstrap < 21.75) + sum(bstrap > 44.29))/1000
```

Therefore, we estimate the p-value to be 0.008. (In other words, in 8 times out of 1000, we had a sample average this extreme.)

Because our significance level is 5% and .8% < 5%, we reject the null hypothesis and conclude that Newcomb’s measurements were not consistent with the currently accepted figure.

Note that there were two extreme observations: -44 and -2. The t-test, because it depends on the sample average, is notoriously susceptible to being influenced by extreme observations. Let’s take those values out and see what happens:

```{r}
betterspeed <- speed[-c(2,18)]
betterspeed
mean(betterspeed)
qqnorm(betterspeed)
qqline(betterspeed)
```

Note that things look much more normal. We can still do our bootstrap test:

```{r}
set.seed(1234)
newspeed <- betterspeed - mean(betterspeed) + 33.02
mean(betterspeed)
bstrap <- c()
for (i in 1:10000){
  bstrap <- c(bstrap, mean(sample(newspeed, 18, replace=T)))
}
```

Now you’ll see that the observed value of our test statistic is 26.7222 (the mean of betterspeed above).

Our pvalue is now the probability of seeing something more than (33.02 - 26.722) = 6.298 units away from 33.02. We calculate this as:

```{r}
(sum(bstrap < 26.722) + sum(bstrap > 39.218))/10000
```

It’s so extreme, that in 1000 repetitions, we never see numbers that extreme.
What if we used the t-test? Since the data now look normal, there’s no reason not to.

```{r}
t.test(betterspeed, alternative="two.sided", mu=33.02)
```

The p value from the t-test is very low (p = 3e-4 = 0.0003), so you would still conclude that Newcomb's measurements were below the present day, accepted value of 33.02.

The bootstrap approach did not require removing data, whereas performing the t-test required assumptions of normality, which led to us removing those potential outlier data points.  


___

## Exercise 4: Hypothesis testing using simulation

A class of 315 students was asked to think of any two-digit number and write it down (secretly).  The numbers were collected and analysed for whether they occurred at random or not.

Since there are 90 different possible two-digit numbers, but only 315 students, we are bound to have under-representation of certain numbers.  Normal contingency table analysis would require at least 5 observations per cell, but 315/90 = 3.5, which suggests that a completely random process would only demonstrate ~3-4 observations per two digit bumber.

We can use hypothesis testing using simulation to determine whether all two-digit numbers occur with equal probability when people choose two-digit numbers haphazardly.

First read and examine data. Each row is the two-digit number chosen by a different  individual.

```{r}
d<-read.csv("haphazard.csv")
str(d)
```

Plot the frequency distribution of two-digit numbers chosen by volunteers. 

```{r}
hist(d$numberSelected, right = FALSE, breaks = seq(10,100, by = 1), 
	las = 1, col = "firebrick", xlab = "Number thought of by volunteer", 
	ylim=c(0,50), xlim=c(0,100))
```
Frequency table of the two-digit numbers chosen by volunteers. Notice that many numbers between 10 and 99 are not represented in the data. To make sure that numbers not chosen are nevertheless included in the calculations, we must convert the variable to a factor and specify all possible categories.

```{r}
table(d$numberSelected)
factorData <- factor(d$numberSelected, levels = 10:99)
observedFreq <- table(factorData)
observedFreq
plot(observedFreq)
```

Now, calculate the observed value of the test statistic. Here, we are using the $\chi^2$ goodness-of-fit statistic. We are lazy and use the chisq.test function to do the calculations. R will give you a warning because of the low expected frequencies. This is exactly why we need to use simulation instead of the $\chi^2$ goodness-of-fit-test. 

```{r}
chiData <- chisq.test(observedFreq)$statistic
chiData
```

We'll need to know the sample size (number of rows, in this case) to run the simulation.

```{r}
n <- nrow(d)
n
```

Simulate a single random sample of n numbers from 10 to 99 (with replacement), where n is the number of individuals in the sample.

```{r}
randomTwoDigit <- sample(10:99, size = n, replace = TRUE)
randomTwoDigit
hist(randomTwoDigit, breaks=20, xlim=c(0,100))
```

Calculate the test statistic for this single simulated sample $\chi^2$. Remember to convert to a factor to make sure that all categories are included in the calculations. Under the null hypothesis, the expected frequency is the same for all numbers.

```{r}
randomTwoDigit <- factor(randomTwoDigit, levels = 10:99)
observedFreq <- table(randomTwoDigit)
expectedFreq <- n/length(10:99)
chiSim <- sum( (observedFreq - expectedFreq)^2 / expectedFreq )
chiSim

```

Or use the chisq.test function:

```{r}
chisq.test(observedFreq)$statistic
```

Repeat this process many times to obtain the distribution for your test statistic. The following is a loop repeated nSim times.  In each iteration i, a random sample of numbers is simulated, a frequency table is calculated, and the $\chi^2$ statistic is computed. The statistic is saved in the ith element of the vector results.

```{r}
nSim <- 10000
results <- c()
for(i in 1:nSim){
	randomTwoDigit <- sample(10:99, size = n, replace = TRUE)
	randomTwoDigit <- factor(randomTwoDigit, levels = 10:99)
	simFreq <- table(randomTwoDigit)
	expectedFreq <- n/length(10:99)
	results[i] <- sum( (simFreq - expectedFreq)^2 / expectedFreq )
	}
```

Plot the frequency distribution of $\chi^2$ values from the simulation. This is the 
simulated null distribution for the test statistic.

```{r}
hist(results, right = FALSE, breaks = 500)
```

The p-value is the fraction of similated chisquare values equalling or exceededing the observed value of the test statistic performed on the original data.

```{r}
P <- sum(results >= chiData)/nSim
P
```

Even with 10000 simulations, none of them produce a $\chi^2$ statistic as large as the observed one.  Thus, the p-value is <1/10000 or <1e-4.

R also has a built-in method to simulate the null distribution and calculate the 
P-value. Using it simply involves providing a couple of arguments to the chisq.test function. B is the number of iterations desired. If you don't provide the probabilities of each outcome under the null hypothesis as an argument, R assumes that all the categories are equiprobable under the nuill hypothesis.

```{r}
factorData <- factor(d$numberSelected, levels = 10:99)
observedFreq <- table(factorData)
chisq.test(observedFreq, simulate.p.value = TRUE, B = 10000)
```

<br> 

___

## Exercise 5 : Chimp language centers

Obtain a bootstrap estimate of the median asymmetry score for Brodmann’s area 44 in chimpanzees.

Read and examine the data.

```{r}
chimp <- read.csv("chimp.csv")
head(chimp)
```

Histogram of asymmetry scores of the 20 chimps.

```{r}
hist(chimp$asymmetryScore, breaks = seq(-.5, 1.25, by = 0.25), 
	right = FALSE, col = "firebrick", las = 1)
```

Estimate the median asymmetry score from the data.

```{r}
median(chimp$asymmetryScore)
```
	
Obtain a single bootstrap replicate. This involves sampling (with replacement) from the actual data. Calculate the median of this bootstrap replicate.

```{r}
bootSample <- sample(chimp$asymmetryScore, size = nrow(chimp), replace = TRUE)
median(bootSample)
```

Repeat this process many times. The following is a loop repeated B times. In each iteration, the bootstrap replicate estimate (median) is recalculated and saved in the results vector bootMedian.
```{r}
B <- 10000
bootMedian <- vector()
for(i in 1:B){
	bootSample <- sample(chimp$asymmetryScore, size = nrow(chimp), replace = TRUE)
	bootMedian[i] <- median(bootSample)
	}
```

Draw a histogram of the bootstrap replicate estimates for median asymmetry. Note: your results won't be the identical to the one here, because 10,000 random samples is not large enough to obtain the sampling distribution with extreme accuracy. Increase B in the above loop for greater accuracy.

```{r}
hist(bootMedian, breaks = 100, right = FALSE, col = "firebrick", las = 1)
```

Calculate the mean of the bootstrap replicate estimates.

```{r}
mean(bootMedian)
```

The bootstrap standard error is the standard deviation of the bootstrap replicate estimates.

```{r}
sd(bootMedian)
```

Use the percentiles of the bootstrap replicate estimates to obtain a bootstrap 95% confidence interval for the population median.

```{r}
quantile(bootMedian, probs = c(0.025, 0.975))
```

Use the boot package for bootstrap estimation. 

```{r}
library(boot)
```

To begin, we need to define a new R function to tell the boot package what statistic we want to estimate, which is the median in this example. Let's call the new function boot.median. The syntax required by the package is not too complex, but our function must include a counter as an argument (as usual, we call it i).

```{r}
boot.median <- function(x, i){median(x[i])}
```

Use the boot function of the boot package to obtain many bootstrap replicate estimates and calculate the bootstrap standard error.

```{r}
bootResults <- boot(chimp$asymmetryScore, boot.median, R = 10000)
bootResults
```

Draw a histogram of the bootstrap replicate estimates for median asymmetry.

```{r}
hist(bootResults$t, breaks = 100, right = FALSE, col = "firebrick", las = 1)
```

Get the bootstrap 95% confidence interval for the population median. We show two methods. The percentile method is already familiar, but boot also provides an improved method, called "bias corrected and accelerated".

```{r}
boot.ci(bootResults, type = "perc")
boot.ci(bootResults, type = "bca")
```


## Exercise 6: Sexual cannibalism in crickets

Bootstrap estimation of the difference between medians of two groups, using times to mating (in hours) of female sagebrush crickets that were either starved or fed. 

Use the boot package in R to address this dataset.

First, read and inspect the data.

```{r}
d<-read.csv("cannibalism.csv")
str(d)
boxplot(timeToMating~feedingStatus, data=d)
```

Calculate the observed difference between the sample medians of the two groups.

```{r}
twoMedians <- tapply(d$timeToMating, d$feedingStatus, median)
diffMedian <- twoMedians[2] -  twoMedians[1]
diffMedian
```

Define a new function to calculate the difference between the medians of the bootstrap replicate estimates. Let's call it boot.diffMedian.

```{r}
boot.diffMedian <- function(x, i){
	twoMedians <- tapply(x$timeToMating[i], x$feedingStatus[i], median)
	diffMedian <- twoMedians[2] -  twoMedians[1]
	}
```

Load the boot package, obtain the many bootstrap replicate estimates and get the bootstrap standard error.

```{r}
library(boot)
bootResults <- boot(d, boot.diffMedian, R = 10000)
mean(bootResults$t)
bootResults
```

Histogram of the bootstrap replicate estimates of the difference between two medians: 

```{r}
hist(bootResults$t, breaks = 20, right = FALSE, col = "firebrick", las = 1)
```

Obtain the bootstrap 95% confidence interval for the difference between the population medians, using the percentile and the "bca" method.

```{r}
boot.ci(bootResults, type = "perc")
boot.ci(bootResults, type = "bca")
```




## Examples and explanations were derived from the following sources:

Whitlock and Schluter

https://stats.idre.ucla.edu/r/library/r-library-introduction-to-bootstrapping/
